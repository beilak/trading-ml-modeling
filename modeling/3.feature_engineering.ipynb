{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2e8010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beilakaliev/Library/Caches/pypoetry/virtualenvs/ml-modeling-TB8BmMSm-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11874937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ШАГ 1: ручная карта подтвержденных сплитов ---\n",
    "def load_split_map(filepath: str) -> dict:\n",
    "    \"\"\"Загружает карту сплитов из JSON-файла.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            split_map = json.load(f)\n",
    "        print(f\"Карта сплитов успешно загружена из: {filepath}\")\n",
    "        return split_map\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Предупреждение: Файл с картой сплитов не найден по пути {filepath}. Корректировка не будет произведена.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"ОШИКА: Не удалось прочитать JSON-файл {filepath}. Проверьте его формат.\")\n",
    "        return {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc1d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# РАСШИРЕННЫЙ ФИЧАИНЖИНИРИНГ (ПОЛНАЯ ИНТЕГРИРОВАННАЯ ВЕРСИЯ)\n",
    "# ==============================================================================\n",
    "\n",
    "def add_features_extended(df: pd.DataFrame, split_map: dict):\n",
    "    \"\"\"\n",
    "    Добавляет в DataFrame расширенный набор технических индикаторов и статистических признаков.\n",
    "    \"\"\"\n",
    "    print(\"Начало расширенного фичаинжиниринга...\")\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values(by=['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "    # --- 1. Признаки тренда (Trend Features) ---\n",
    "    print(\"Расчет индикаторов тренда...\")\n",
    "    sma_periods = [3, 5, 7, 10, 15, 20, 30, 40, 50, 70, 100, 150, 200]\n",
    "    for i in sma_periods:\n",
    "        df[f\"sma_{i}\"] = df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(i).mean())\n",
    "\n",
    "    macd = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.macd(x['Close'], fast=12, slow=26, signal=9))\n",
    "    df = pd.concat([df, macd], axis=1)\n",
    "    adx = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.adx(x['High'], x['Low'], x['Close'], length=14))\n",
    "    df = pd.concat([df, adx], axis=1)\n",
    "\n",
    "    # --- 2. Признаки моментума (Momentum Features) ---\n",
    "    print(\"Расчет индикаторов моментума...\")\n",
    "    rsi_periods = [5, 7, 14, 21, 30, 50]\n",
    "    for i in rsi_periods:\n",
    "        df[f\"rsi_{i}\"] = df.groupby('Ticker')['Close'].transform(lambda x: ta.rsi(x, length=i))\n",
    "\n",
    "    stoch = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.stoch(x['High'], x['Low'], x['Close'], k=14, d=3, smooth_k=3))\n",
    "    df = pd.concat([df, stoch], axis=1)\n",
    "    willr_periods = [5, 7, 14, 21, 30]\n",
    "    for i in willr_periods:\n",
    "        df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
    "\n",
    "    # --- 3. Признаки волатильности (Volatility Features) ---\n",
    "    print(\"Расчет индикаторов волатильности...\")\n",
    "    atr_periods = [5, 7, 14, 21]\n",
    "    for i in atr_periods:\n",
    "        atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n",
    "        df[f\"atr_{i}\"] = atr\n",
    "\n",
    "    bollinger = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.bbands(x['Close'], length=20, std=2))\n",
    "    \n",
    "    df = pd.concat([df, bollinger], axis=1)\n",
    "    df['bb_width_norm'] = (df['BBU_20_2.0_2.0'] - df['BBL_20_2.0_2.0']) / (df['BBM_20_2.0_2.0'] + 1e-9)\n",
    "\n",
    "    # --- 4. Признаки объема (Volume Features) ---\n",
    "    print(\"Расчет индикаторов объема...\")\n",
    "    vol_sma_periods = [5, 7, 14, 20, 30]\n",
    "    for i in vol_sma_periods:\n",
    "        df[f\"vol_sma_{i}\"] = df.groupby('Ticker')['Volume'].transform(lambda x: x.rolling(i).mean())\n",
    "        df[f'relative_volume_{i}'] = df['Volume'] / (df[f\"vol_sma_{i}\"] + 1e-9)\n",
    "\n",
    "    print(\"Расчет и нормализация OBV...\")\n",
    "    obv_series = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.obv(x['Close'], x['Volume']))\n",
    "    df['obv'] = obv_series \n",
    "    for i in vol_sma_periods:\n",
    "        obv_sma_col = f'obv_sma_{i}'\n",
    "        df[obv_sma_col] = df.groupby('Ticker')['obv'].transform(lambda x: x.rolling(i).mean())\n",
    "        df[f'obv_relative_{i}'] = df['obv'] / (df[obv_sma_col] + 1e-9)        \n",
    "        df[f'obv_trend_{i}'] = df['obv'] - df[obv_sma_col]\n",
    "\n",
    "\n",
    "    df['turnover'] = df['Close'] * df['Volume']\n",
    "    for i in vol_sma_periods: # Используем те же периоды для сопоставимости\n",
    "        df[f\"turnover_sma_{i}\"] = df.groupby('Ticker')['turnover'].transform(lambda x: x.rolling(i).mean())\n",
    "        df[f'relative_turnover_{i}'] = df['turnover'] / (df[f\"turnover_sma_{i}\"] + 1e-9)\n",
    "    df.drop(columns=['turnover'], inplace=True)\n",
    "\n",
    "    # --- 5. Признаки свечей и меж-дневной динамики ---\n",
    "    print(\"Расчет признаков свечей и меж-дневной динамики...\")\n",
    "    df['day_range_norm'] = (df['High'] - df['Low']) / (df['Close'] + 1e-9)\n",
    "    df['intraday_move_norm'] = (df['Close'] - df['Open']) / (df['Close'] + 1e-9)\n",
    "    df['upper_wick_norm'] = (df['High'] - df[['Open', 'Close']].max(axis=1)) / (df['Close'] + 1e-9)\n",
    "    df['lower_wick_norm'] = (df[['Open', 'Close']].min(axis=1) - df['Low']) / (df['Close'] + 1e-9)\n",
    "    df['overnight_gap_norm'] = (df['Open'] - df.groupby('Ticker')['Close'].shift(1)) / (df.groupby('Ticker')['Close'].shift(1) + 1e-9)\n",
    "    daily_range = df['High'] - df['Low']\n",
    "    df['range_expansion_ratio'] = daily_range / (df.groupby('Ticker')['High'].shift(1) - df.groupby('Ticker')['Low'].shift(1) + 1e-9)\n",
    "\n",
    "    # --- 6. Признаки взаимодействия Цены и Объема ---\n",
    "    print(\"Расчет признаков взаимодействия Цены и Объема...\")\n",
    "    # Используем relative_volume_20, так как он соответствует периоду Bollinger Bands\n",
    "    df['volume_weighted_move'] = df['intraday_move_norm'] * df['relative_volume_20']\n",
    "    df['daily_return'] = df.groupby('Ticker')['Close'].pct_change()\n",
    "    df['up_day_volume'] = df.apply(lambda row: row['Volume'] if row['daily_return'] > 0 else 0, axis=1)\n",
    "    df['down_day_volume'] = df.apply(lambda row: row['Volume'] if row['daily_return'] <= 0 else 0, axis=1)\n",
    "    \n",
    "    sma_up_vol = df.groupby('Ticker')['up_day_volume'].transform(lambda x: x.rolling(20).mean())\n",
    "    sma_down_vol = df.groupby('Ticker')['down_day_volume'].transform(lambda x: x.rolling(20).mean())\n",
    "    df['up_down_volume_ratio'] = sma_up_vol / (sma_down_vol + 1e-9)\n",
    "    df.drop(columns=['daily_return', 'up_day_volume', 'down_day_volume'], inplace=True)\n",
    "\n",
    "    # --- 7. Статистические признаки ---\n",
    "    print(\"Расчет статистических признаков...\")\n",
    "    df['log_return'] = df.groupby('Ticker')['Close'].transform(lambda x: np.log(x / x.shift(1)))\n",
    "    stat_periods = [7, 14, 21]\n",
    "    for i in stat_periods:\n",
    "        df[f'rolling_std_{i}'] = df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(i).std())\n",
    "        df[f'rolling_skew_{i}'] = df.groupby('Ticker')['log_return'].transform(lambda x: x.rolling(i).skew())\n",
    "        df[f'rolling_kurt_{i}'] = df.groupby('Ticker')['log_return'].transform(lambda x: x.rolling(i).kurt())\n",
    "\n",
    "    # --- 8. Календарные признаки ---\n",
    "    print(\"Добавление календарных признаков...\")\n",
    "    df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['week_of_year'] = df['Date'].dt.isocalendar().week.astype(int)\n",
    "    df['day_of_year'] = df['Date'].dt.dayofyear\n",
    "    df['quarter'] = df['Date'].dt.quarter\n",
    "    df['is_month_start'] = df['Date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['Date'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['Date'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['Date'].dt.is_quarter_end.astype(int)\n",
    "    df['is_year_start'] = df['Date'].dt.is_year_start.astype(int)\n",
    "    df['is_year_end'] = df['Date'].dt.is_year_end.astype(int)\n",
    "    def _get_season(month):\n",
    "        if month in [12, 1, 2]: return 0 # Winter\n",
    "        elif month in [3, 4, 5]: return 1 # Spring\n",
    "        elif month in [6, 7, 8]: return 2 # Summer\n",
    "        else: return 3 # Autumn\n",
    "    df['season'] = df['month'].apply(_get_season)\n",
    "    \n",
    "    # --- 9. Событийные признаки и взаимодействия ---\n",
    "    print(\"Расчет признаков взаимодействия и событий...\")\n",
    "    # Ваш полный набор признаков взаимодействия\n",
    "    for i in sma_periods:\n",
    "        sma_col = f'sma_{i}'\n",
    "        df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
    "        df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
    "        df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
    "        df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
    "\n",
    "    # Признак сплита (событийный)\n",
    "    df['is_split_day'] = 0\n",
    "    for ticker, events in split_map.items():\n",
    "        for event in events:\n",
    "            event_date = pd.to_datetime(event['date'])\n",
    "            idx = df[(df['Ticker'] == ticker) & (df['Date'] == event_date)].index\n",
    "            if not idx.empty:\n",
    "                df.loc[idx, 'is_split_day'] = 1\n",
    "    print(f\"Найдено и отмечено {df['is_split_day'].sum()} дней со сплитами.\")\n",
    "\n",
    "\n",
    "    # --- НОВЫЙ РАЗДЕЛ 10: ПРИЗНАКИ ПЕРЕСЕЧЕНИЯ СКОЛЬЗЯЩИХ СРЕДНИХ ---\n",
    "    print(\"Расчет признаков пересечения скользящих средних...\")\n",
    "\n",
    "        \n",
    "    # Определяем пары для анализа (быстрая, медленная)\n",
    "    sma_cross_pairs = [\n",
    "        (70, 200), # Классическое \"Золотое/Мертвое\" пересечение\n",
    "        (50, 200), \n",
    "        (20, 50),  # Среднесрочное пересечение\n",
    "        # Краткосрочные пересечения\n",
    "        (7, 15),\n",
    "        (3, 10),\n",
    "        (3, 7),\n",
    "    ]\n",
    "\n",
    "    for fast_period, slow_period in sma_cross_pairs:\n",
    "        fast_col = f'sma_{fast_period}'\n",
    "        slow_col = f'sma_{slow_period}'\n",
    "        \n",
    "        # Убедимся, что нужные SMA уже посчитаны\n",
    "        if fast_col not in df.columns or slow_col not in df.columns:\n",
    "            raise ValueError(f\"NO sma {fast_col = }. {slow_col = }\")\n",
    "    \n",
    "        # --- Признак 2: Состояние тренда ---\n",
    "        state_col = f'sma{fast_period}_above_sma{slow_period}'\n",
    "        df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
    "        \n",
    "        # --- Признак 1: Сигнал пересечения ---\n",
    "        signal_col = f'sma{fast_period}_cross_sma{slow_period}'\n",
    "        # Сдвигаем состояние на 1 день назад, чтобы сравнить \"сегодня\" и \"вчера\"\n",
    "        prev_state = df.groupby('Ticker')[state_col].shift(1)\n",
    "        # Пересечение - это когда состояние изменилось (0->1 или 1->0)\n",
    "        df[signal_col] = 0\n",
    "        # Бычье пересечение (+1): было 0, стало 1\n",
    "        df.loc[(df[state_col] == 1) & (prev_state == 0), signal_col] = 1\n",
    "        # Медвежье пересечение (-1): было 1, стало 0\n",
    "        df.loc[(df[state_col] == 0) & (prev_state == 1), signal_col] = -1\n",
    "\n",
    "        # --- Признак 3: Дни с момента пересечения ---\n",
    "        days_since_col = f'days_since_sma{fast_period}_cross_{slow_period}'\n",
    "        # Находим, где были пересечения (не равно 0)\n",
    "        cross_events = df[signal_col].ne(0)\n",
    "        # Создаем группы, которые начинаются с каждого пересечения\n",
    "        cross_groups = cross_events.cumsum()\n",
    "        # Считаем дни внутри каждой группы\n",
    "        df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
    "\n",
    "\n",
    "    # --- 11. Продвинутые сигналы технического анализа ---\n",
    "    print(\"Расчет продвинутых сигналов теханализа...\")\n",
    "\n",
    "    # 1. Сигналы ADX (Average Directional Index)\n",
    "    # Что это: ADX показывает СИЛУ тренда (не направление). Пересечение линий +DI и -DI показывает НАПРАВЛЕНИЕ.    \n",
    "    adx_col = 'ADX_14'\n",
    "    dmp_col = 'DMP_14' # +DI\n",
    "    dmn_col = 'DMN_14' # -DI\n",
    "    \n",
    "    # Убедимся, что колонки существуют\n",
    "    if not (adx_col in df.columns and dmp_col in df.columns and dmn_col in df.columns):\n",
    "        raise ValueError(f\"ERROR no {adx_col = }. {dmp_col = }, { dmn_col = }\")\n",
    "        # Признак \"Сила направленного движения\": ADX, умноженный на знак тренда.\n",
    "        # Знак тренда = +1, если +DI выше -DI (бычий), и -1, если наоборот.\n",
    "    trend_direction = (df[dmp_col] > df[dmn_col]).astype(int) * 2 - 1 # Преобразует True/False в +1/-1\n",
    "    df['adx_trend_strength'] = df[adx_col] * trend_direction\n",
    "\n",
    "    # 2. Сигналы MACD (Moving Average Convergence Divergence)\n",
    "    # Что это: Пересечение линии MACD с ее сигнальной линией - классический сигнал.\n",
    "    # Гистограмма (разница между линиями) показывает силу моментума.\n",
    "    macd_line_col = 'MACD_12_26_9'\n",
    "    signal_line_col = 'MACDs_12_26_9'\n",
    "    hist_col = 'MACDh_12_26_9'\n",
    "\n",
    "    if not (macd_line_col in df.columns and signal_line_col in df.columns):\n",
    "        raise ValueError(f\"NO {macd_line_col = }. {signal_line_col = }\")\n",
    "\n",
    "    # Состояние MACD: +1 если MACD выше сигнальной линии (бычье), -1 если ниже (медвежье)\n",
    "    df['macd_state'] = (df[macd_line_col] > df[signal_line_col]).astype(int) * 2 - 1\n",
    "    \n",
    "    # Сигнал пересечения MACD (+1 = бычье, -1 = медвежье)\n",
    "    prev_macd_state = df.groupby('Ticker')['macd_state'].shift(1)\n",
    "    df['macd_cross_signal'] = 0\n",
    "    df.loc[(df['macd_state'] == 1) & (prev_macd_state == -1), 'macd_cross_signal'] = 1\n",
    "    df.loc[(df['macd_state'] == -1) & (prev_macd_state == 1), 'macd_cross_signal'] = -1\n",
    "    \n",
    "    # Признак \"Ускорение моментума\": растет ли гистограмма?\n",
    "    df['macd_hist_acceleration'] = (df[hist_col] > df.groupby('Ticker')[hist_col].shift(1)).astype(int)\n",
    "\n",
    "\n",
    "    # 3. Сигналы по Полосам Боллинджера (Bollinger Bands)\n",
    "    # Что это: Касание или пробой границ канала - сильный сигнал.\n",
    "    upper_bb_col = 'BBU_20_2.0_2.0'\n",
    "    lower_bb_col = 'BBL_20_2.0_2.0'\n",
    "    \n",
    "    if not (upper_bb_col in df.columns and lower_bb_col in df.columns):\n",
    "        raise ValueError(f\"NO { upper_bb_col =}. {lower_bb_col = }\")\n",
    "    # Признак \"Пробой верхней границы\"\n",
    "    df['bb_upper_breakout'] = (df['Close'] > df[upper_bb_col]).astype(int)\n",
    "    # Признак \"Пробой нижней границы\"\n",
    "    df['bb_lower_breakout'] = (df['Close'] < df[lower_bb_col]).astype(int)\n",
    "    # Положение цены внутри канала (от 0 до 1)\n",
    "    # 0 = на нижней границе, 1 = на верхней границе, >1 = пробой вверх, <0 = пробой вниз\n",
    "    df['bb_percent_b'] = (df['Close'] - df[lower_bb_col]) / (df[upper_bb_col] - df[lower_bb_col] + 1e-9)\n",
    "\n",
    "\n",
    "    print(\"Генерация признаков на основе ставки ЦБ...\")    \n",
    "    if 'cbr_rate' in df.columns:\n",
    "        # 1. Величина изменения ставки (рассчитывается внутри каждой группы тикеров)\n",
    "        # .transform() применяет операцию к группе и возвращает результат того же размера,\n",
    "        # что и исходный DataFrame, избегая смешивания данных.\n",
    "        rate_change = df.groupby('Ticker')['cbr_rate'].transform(lambda x: x.replace(-1, np.nan).diff())        \n",
    "        df['cbr_rate_change_value'] = rate_change.fillna(0)\n",
    "\n",
    "        # 2. Факт изменения ставки (1 - было изменение, 0 - не было)\n",
    "        df['cbr_rate_change_flag'] = (df['cbr_rate_change_value'] != 0).astype(int)\n",
    "        \n",
    "    else:\n",
    "        print(\"Предупреждение: Колонка 'cbr_rate' не найдена. Признаки на ее основе не будут созданы.\")\n",
    "        raise Exception\n",
    "\n",
    "\n",
    "    # ---  Признаки Моментума и Относительной Силы ---\n",
    "    print(\"Расчет признаков моментума и относительной силы...\")\n",
    "\n",
    "    # Периоды для анализа\n",
    "    momentum_periods = [3, 5, 7, 10, 14, 21, 30, 60, 100]\n",
    "\n",
    "    for n in momentum_periods:\n",
    "        # Рассчитывается внутри каждого тикера\n",
    "        df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
    "            lambda x: x.pct_change(periods=n)\n",
    "        )        \n",
    "        # --- Моментум, скорректированный на риск (Sharpe Ratio тренда) ---\n",
    "        # log_return уже должен быть рассчитан в секции статистических признаков\n",
    "        if not ('log_return' in df.columns):\n",
    "            raise ValueError(\"No log_return\")\n",
    "        returns_grouped = df.groupby('Ticker')['log_return']\n",
    "        mean_returns = returns_grouped.transform(lambda x: x.rolling(n).mean())\n",
    "        std_returns = returns_grouped.transform(lambda x: x.rolling(n).std())\n",
    "        df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
    "\n",
    "    # ---: Кросс-секционный моментум (Ранжирование) ---\n",
    "    # Этот расчет должен идти после цикла, так как он работает со всеми тикерами одновременно\n",
    "    # для каждой конкретной даты.\n",
    "    print(\"Расчет кросс-секционного ранжирования по моментуму...\")\n",
    "    for n in momentum_periods:\n",
    "        # groupby('Date') - ключевой шаг. Ранжируем акции ВНУТРИ каждого дня.\n",
    "        # rank(pct=True) - преобразует ранг в процентиль (от 0.0 до 1.0), \n",
    "        # что является лучшей практикой для ML моделей.\n",
    "        df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- 10. Финальная очистка от NaN ---\n",
    "    print(\"Очистка данных от NaN...\")\n",
    "    # Находим самый длинный период из всех использованных\n",
    "    longest_period = max(sma_periods)\n",
    "    print(f\"Удаление первых {longest_period} строк для каждого тикера для прогрева индикаторов...\")\n",
    "    # Отбрасываем N первых строк для КАЖДОГО тикера\n",
    "    df = df.groupby('Ticker', group_keys=False).apply(lambda x: x.iloc[longest_period:])\n",
    "    # Дополнительно убираем строки, если где-то остались NaN (например, из-за .shift() в новых признаках)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    print(\"Расширенный фичаинжиниринг завершен.\")\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b48a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка сырых данных из: ../data/moex_normalized_data.csv\n",
      "Данные успешно загружены.\n",
      "Карта сплитов успешно загружена из: config/splits.json\n",
      "Начало расширенного фичаинжиниринга...\n",
      "Расчет индикаторов тренда...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:19: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  macd = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.macd(x['Close'], fast=12, slow=26, signal=9))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:21: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  adx = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.adx(x['High'], x['Low'], x['Close'], length=14))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет индикаторов моментума...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stoch = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.stoch(x['High'], x['Low'], x['Close'], k=14, d=3, smooth_k=3))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:34: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:34: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:34: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:34: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:34: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет индикаторов волатильности...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:43: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bollinger = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.bbands(x['Close'], length=20, std=2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет индикаторов объема...\n",
      "Расчет и нормализация OBV...\n",
      "Расчет признаков свечей и меж-дневной динамики...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:56: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  obv_series = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.obv(x['Close'], x['Volume']))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:85: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df['daily_return'] = df.groupby('Ticker')['Close'].pct_change()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет признаков взаимодействия Цены и Объема...\n",
      "Расчет статистических признаков...\n",
      "Добавление календарных признаков...\n",
      "Расчет признаков взаимодействия и событий...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_month_start'] = df['Date'].dt.is_month_start.astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:111: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_month_end'] = df['Date'].dt.is_month_end.astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:112: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_quarter_start'] = df['Date'].dt.is_quarter_start.astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_quarter_end'] = df['Date'].dt.is_quarter_end.astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_year_start'] = df['Date'].dt.is_year_start.astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_year_end'] = df['Date'].dt.is_year_end.astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['season'] = df['month'].apply(_get_season)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:134: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_split_day'] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['adx_trend_strength'] = df[adx_col] * trend_direction\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:220: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['macd_state'] = (df[macd_line_col] > df[signal_line_col]).astype(int) * 2 - 1\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:224: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['macd_cross_signal'] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:229: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['macd_hist_acceleration'] = (df[hist_col] > df.groupby('Ticker')[hist_col].shift(1)).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:240: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['bb_upper_breakout'] = (df['Close'] > df[upper_bb_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:242: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['bb_lower_breakout'] = (df['Close'] < df[lower_bb_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['bb_percent_b'] = (df['Close'] - df[lower_bb_col]) / (df[upper_bb_col] - df[lower_bb_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:254: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['cbr_rate_change_value'] = rate_change.fillna(0)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:257: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['cbr_rate_change_flag'] = (df['cbr_rate_change_value'] != 0).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:272: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено и отмечено 4 дней со сплитами.\n",
      "Расчет признаков пересечения скользящих средних...\n",
      "Расчет продвинутых сигналов теханализа...\n",
      "Генерация признаков на основе ставки ЦБ...\n",
      "Расчет признаков моментума и относительной силы...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:272: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:272: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:272: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:272: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:272: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:272: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:272: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:272: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:292: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:292: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:292: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:292: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет кросс-секционного ранжирования по моментуму...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:292: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:292: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:292: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:292: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:292: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Очистка данных от NaN...\n",
      "Удаление первых 200 строк для каждого тикера для прогрева индикаторов...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_68944/3810807164.py:303: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Ticker', group_keys=False).apply(lambda x: x.iloc[longest_period:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расширенный фичаинжиниринг завершен.\n",
      "\n",
      "--- Фильтрация признаков по списку исключений ---\n",
      "Загружен список из 5 признаков для исключения.\n",
      "Будет исключено 5 признаков: ['is_year_start', 'is_year_end', 'is_split_day', 'obv', 'cbr_rate']\n",
      "\n",
      "--- DataFrame с признаками ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 173989 entries, 0 to 173988\n",
      "Columns: 220 entries, Date to momentum_rank_100d\n",
      "dtypes: datetime64[ns](1), float64(184), int32(4), int64(30), object(1)\n",
      "memory usage: 289.4+ MB\n",
      "None\n",
      "\n",
      "Сохранение данных с признаками в файл: ../data/moex_with_features.csv\n",
      "Скрипт фичаинжиниринга выполнен успешно!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_folder = \"../data/\"\n",
    "normalized_data_filename = 'moex_normalized_data.csv'\n",
    "features_data_filename = 'moex_with_features.csv'\n",
    "\n",
    "config_folder = \"config/\"\n",
    "split_map_filename = os.path.join(config_folder, \"splits.json\")\n",
    "\n",
    "exclude_list_filename = os.path.join(config_folder, \"feature_exclude_list.json\") # Путь к \"черному списку\"\n",
    "\n",
    "# --- ШАГ 1: Загрузка сырых данных ---\n",
    "print(f\"Загрузка сырых данных из: {data_folder + normalized_data_filename}\")\n",
    "try:\n",
    "    raw_data = pd.read_csv(data_folder + normalized_data_filename)\n",
    "    print(\"Данные успешно загружены.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ОШИБКА: Файл не найден. Убедитесь, что скрипт сохранения сырых данных был запущен.\")\n",
    "    exit()\n",
    "\n",
    "split_map = load_split_map(split_map_filename)\n",
    "\n",
    "# --- ШАГ 2: Добавление признаков ---\n",
    "# tqdm.pandas(desc=\"Расчет индикаторов\")\n",
    "# data_with_features = add_features(raw_data)\n",
    "data_with_features = add_features_extended(raw_data, split_map)\n",
    "\n",
    "\n",
    "\n",
    "# --- ЭТАП 2: ФИЛЬТРАЦИЯ ПО \"ЧЕРНОМУ СПИСКУ\" ---\n",
    "print(\"\\n--- Фильтрация признаков по списку исключений ---\")\n",
    "\n",
    "features_to_exclude = []\n",
    "try:\n",
    "    with open(exclude_list_filename, 'r', encoding='utf-8') as f:\n",
    "        features_to_exclude = json.load(f)\n",
    "    print(f\"Загружен список из {len(features_to_exclude)} признаков для исключения.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ИНФО: Файл исключений '{exclude_list_filename}' не найден. Все признаки будут сохранены.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"ПРЕДУПРЕЖДЕНИЕ: Не удалось прочитать JSON из '{exclude_list_filename}'. Все признаки будут сохранены.\")\n",
    "\n",
    "if features_to_exclude:\n",
    "    # Находим, какие из признаков в списке реально есть в DataFrame\n",
    "    cols_to_drop = [col for col in features_to_exclude if col in data_with_features.columns]\n",
    "    \n",
    "    if cols_to_drop:\n",
    "        print(f\"Будет исключено {len(cols_to_drop)} признаков: {cols_to_drop}\")\n",
    "        final_df = data_with_features.drop(columns=cols_to_drop)\n",
    "    else:\n",
    "        print(\"Ни один из признаков в списке исключений не найден в DataFrame.\")\n",
    "        final_df = data_with_features\n",
    "else:\n",
    "    final_df = data_with_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- DataFrame с признаками ---\")\n",
    "print(final_df.info())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- ШАГ 3: Сохранение результата ---\n",
    "print(f\"\\nСохранение данных с признаками в файл: {data_folder + features_data_filename}\")\n",
    "final_df.to_csv(data_folder + features_data_filename, index=False)\n",
    "\n",
    "print(\"Скрипт фичаинжиниринга выполнен успешно!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efd238e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>cbr_rate</th>\n",
       "      <th>sma_3</th>\n",
       "      <th>sma_5</th>\n",
       "      <th>...</th>\n",
       "      <th>momentum_sharpe_100d</th>\n",
       "      <th>momentum_rank_3d</th>\n",
       "      <th>momentum_rank_5d</th>\n",
       "      <th>momentum_rank_7d</th>\n",
       "      <th>momentum_rank_10d</th>\n",
       "      <th>momentum_rank_14d</th>\n",
       "      <th>momentum_rank_21d</th>\n",
       "      <th>momentum_rank_30d</th>\n",
       "      <th>momentum_rank_60d</th>\n",
       "      <th>momentum_rank_100d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-27</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>17.04</td>\n",
       "      <td>17.30</td>\n",
       "      <td>16.62</td>\n",
       "      <td>17.30</td>\n",
       "      <td>9700300.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.313333</td>\n",
       "      <td>17.084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024654</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.618182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-03-30</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>17.21</td>\n",
       "      <td>17.47</td>\n",
       "      <td>17.08</td>\n",
       "      <td>17.40</td>\n",
       "      <td>13542700.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.246667</td>\n",
       "      <td>17.246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012765</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.490909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>17.37</td>\n",
       "      <td>17.93</td>\n",
       "      <td>17.10</td>\n",
       "      <td>17.90</td>\n",
       "      <td>17961200.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.533333</td>\n",
       "      <td>17.448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013097</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.509091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>17.90</td>\n",
       "      <td>18.02</td>\n",
       "      <td>17.52</td>\n",
       "      <td>17.75</td>\n",
       "      <td>12653600.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.683333</td>\n",
       "      <td>17.478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017035</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.381818</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.490909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-02</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>17.75</td>\n",
       "      <td>18.00</td>\n",
       "      <td>17.12</td>\n",
       "      <td>17.85</td>\n",
       "      <td>21043800.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.833333</td>\n",
       "      <td>17.640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019174</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.527273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173984</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4312.00</td>\n",
       "      <td>4312.00</td>\n",
       "      <td>4171.50</td>\n",
       "      <td>4200.00</td>\n",
       "      <td>673681.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4273.500000</td>\n",
       "      <td>4320.100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022856</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.628205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173985</th>\n",
       "      <td>2025-09-15</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4220.50</td>\n",
       "      <td>4222.00</td>\n",
       "      <td>4120.00</td>\n",
       "      <td>4150.00</td>\n",
       "      <td>473297.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4215.666667</td>\n",
       "      <td>4269.100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019336</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.525641</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.641026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173986</th>\n",
       "      <td>2025-09-16</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4157.50</td>\n",
       "      <td>4183.00</td>\n",
       "      <td>4058.50</td>\n",
       "      <td>4127.50</td>\n",
       "      <td>493769.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4159.166667</td>\n",
       "      <td>4219.600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023737</td>\n",
       "      <td>0.474359</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173987</th>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4128.00</td>\n",
       "      <td>4164.00</td>\n",
       "      <td>4100.00</td>\n",
       "      <td>4161.00</td>\n",
       "      <td>448318.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4146.166667</td>\n",
       "      <td>4187.100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029947</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.474359</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.525641</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.679487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173988</th>\n",
       "      <td>2025-09-18</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4161.50</td>\n",
       "      <td>4174.50</td>\n",
       "      <td>4100.50</td>\n",
       "      <td>4108.50</td>\n",
       "      <td>544863.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4132.333333</td>\n",
       "      <td>4149.400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025359</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>0.525641</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173989 rows × 225 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date Ticker     Open     High      Low    Close      Volume  \\\n",
       "0      2015-03-27   AFKS    17.04    17.30    16.62    17.30   9700300.0   \n",
       "1      2015-03-30   AFKS    17.21    17.47    17.08    17.40  13542700.0   \n",
       "2      2015-03-31   AFKS    17.37    17.93    17.10    17.90  17961200.0   \n",
       "3      2015-04-01   AFKS    17.90    18.02    17.52    17.75  12653600.0   \n",
       "4      2015-04-02   AFKS    17.75    18.00    17.12    17.85  21043800.0   \n",
       "...           ...    ...      ...      ...      ...      ...         ...   \n",
       "173984 2025-09-12   YDEX  4312.00  4312.00  4171.50  4200.00    673681.0   \n",
       "173985 2025-09-15   YDEX  4220.50  4222.00  4120.00  4150.00    473297.0   \n",
       "173986 2025-09-16   YDEX  4157.50  4183.00  4058.50  4127.50    493769.0   \n",
       "173987 2025-09-17   YDEX  4128.00  4164.00  4100.00  4161.00    448318.0   \n",
       "173988 2025-09-18   YDEX  4161.50  4174.50  4100.50  4108.50    544863.0   \n",
       "\n",
       "        cbr_rate        sma_3     sma_5  ...  momentum_sharpe_100d  \\\n",
       "0           14.0    17.313333    17.084  ...              0.024654   \n",
       "1           14.0    17.246667    17.246  ...              0.012765   \n",
       "2           14.0    17.533333    17.448  ...              0.013097   \n",
       "3           14.0    17.683333    17.478  ...              0.017035   \n",
       "4           14.0    17.833333    17.640  ...              0.019174   \n",
       "...          ...          ...       ...  ...                   ...   \n",
       "173984      18.0  4273.500000  4320.100  ...             -0.022856   \n",
       "173985      18.0  4215.666667  4269.100  ...             -0.019336   \n",
       "173986      18.0  4159.166667  4219.600  ...             -0.023737   \n",
       "173987      18.0  4146.166667  4187.100  ...             -0.029947   \n",
       "173988      18.0  4132.333333  4149.400  ...             -0.025359   \n",
       "\n",
       "        momentum_rank_3d  momentum_rank_5d  momentum_rank_7d  \\\n",
       "0               0.872727          0.890909          0.890909   \n",
       "1               0.254545          0.909091          0.872727   \n",
       "2               0.781818          0.909091          0.927273   \n",
       "3               0.418182          0.381818          0.872727   \n",
       "4               0.500000          0.509091          0.690909   \n",
       "...                  ...               ...               ...   \n",
       "173984          0.410256          0.346154          0.397436   \n",
       "173985          0.448718          0.282051          0.435897   \n",
       "173986          0.474359          0.371795          0.384615   \n",
       "173987          0.551282          0.512821          0.397436   \n",
       "173988          0.602564          0.576923          0.448718   \n",
       "\n",
       "        momentum_rank_10d  momentum_rank_14d  momentum_rank_21d  \\\n",
       "0                1.000000           0.927273           0.672727   \n",
       "1                0.981818           1.000000           0.618182   \n",
       "2                0.927273           1.000000           0.672727   \n",
       "3                0.872727           0.981818           0.654545   \n",
       "4                0.818182           0.981818           0.727273   \n",
       "...                   ...                ...                ...   \n",
       "173984           0.641026           0.410256           0.756410   \n",
       "173985           0.435897           0.397436           0.628205   \n",
       "173986           0.346154           0.615385           0.435897   \n",
       "173987           0.474359           0.487179           0.551282   \n",
       "173988           0.525641           0.653846           0.602564   \n",
       "\n",
       "        momentum_rank_30d  momentum_rank_60d  momentum_rank_100d  \n",
       "0                0.636364           0.872727            0.618182  \n",
       "1                0.490909           0.909091            0.490909  \n",
       "2                0.527273           0.927273            0.509091  \n",
       "3                0.109091           0.909091            0.490909  \n",
       "4                0.400000           0.836364            0.527273  \n",
       "...                   ...                ...                 ...  \n",
       "173984           0.538462           0.653846            0.628205  \n",
       "173985           0.525641           0.628205            0.641026  \n",
       "173986           0.538462           0.576923            0.653846  \n",
       "173987           0.525641           0.551282            0.679487  \n",
       "173988           0.423077           0.589744            0.730769  \n",
       "\n",
       "[173989 rows x 225 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36991423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! All columns\n",
      "0 Date\n",
      "1 Ticker\n",
      "2 Open\n",
      "3 High\n",
      "4 Low\n",
      "5 Close\n",
      "6 Volume\n",
      "7 cbr_rate\n",
      "8 sma_3\n",
      "9 sma_5\n",
      "10 sma_7\n",
      "11 sma_10\n",
      "12 sma_15\n",
      "13 sma_20\n",
      "14 sma_30\n",
      "15 sma_40\n",
      "16 sma_50\n",
      "17 sma_70\n",
      "18 sma_100\n",
      "19 sma_150\n",
      "20 sma_200\n",
      "21 MACD_12_26_9\n",
      "22 MACDh_12_26_9\n",
      "23 MACDs_12_26_9\n",
      "24 ADX_14\n",
      "25 ADXR_14_2\n",
      "26 DMP_14\n",
      "27 DMN_14\n",
      "28 rsi_5\n",
      "29 rsi_7\n",
      "30 rsi_14\n",
      "31 rsi_21\n",
      "32 rsi_30\n",
      "33 rsi_50\n",
      "34 STOCHk_14_3_3\n",
      "35 STOCHd_14_3_3\n",
      "36 STOCHh_14_3_3\n",
      "37 willr_5\n",
      "38 willr_7\n",
      "39 willr_14\n",
      "40 willr_21\n",
      "41 willr_30\n",
      "42 atr_5\n",
      "43 atr_7\n",
      "44 atr_14\n",
      "45 atr_21\n",
      "46 BBL_20_2.0_2.0\n",
      "47 BBM_20_2.0_2.0\n",
      "48 BBU_20_2.0_2.0\n",
      "49 BBB_20_2.0_2.0\n",
      "50 BBP_20_2.0_2.0\n",
      "51 bb_width_norm\n",
      "52 vol_sma_5\n",
      "53 relative_volume_5\n",
      "54 vol_sma_7\n",
      "55 relative_volume_7\n",
      "56 vol_sma_14\n",
      "57 relative_volume_14\n",
      "58 vol_sma_20\n",
      "59 relative_volume_20\n",
      "60 vol_sma_30\n",
      "61 relative_volume_30\n",
      "62 obv\n",
      "63 obv_sma_5\n",
      "64 obv_relative_5\n",
      "65 obv_trend_5\n",
      "66 obv_sma_7\n",
      "67 obv_relative_7\n",
      "68 obv_trend_7\n",
      "69 obv_sma_14\n",
      "70 obv_relative_14\n",
      "71 obv_trend_14\n",
      "72 obv_sma_20\n",
      "73 obv_relative_20\n",
      "74 obv_trend_20\n",
      "75 obv_sma_30\n",
      "76 obv_relative_30\n",
      "77 obv_trend_30\n",
      "78 turnover_sma_5\n",
      "79 relative_turnover_5\n",
      "80 turnover_sma_7\n",
      "81 relative_turnover_7\n",
      "82 turnover_sma_14\n",
      "83 relative_turnover_14\n",
      "84 turnover_sma_20\n",
      "85 relative_turnover_20\n",
      "86 turnover_sma_30\n",
      "87 relative_turnover_30\n",
      "88 day_range_norm\n",
      "89 intraday_move_norm\n",
      "90 upper_wick_norm\n",
      "91 lower_wick_norm\n",
      "92 overnight_gap_norm\n",
      "93 range_expansion_ratio\n",
      "94 volume_weighted_move\n",
      "95 up_down_volume_ratio\n",
      "96 log_return\n",
      "97 rolling_std_7\n",
      "98 rolling_skew_7\n",
      "99 rolling_kurt_7\n",
      "100 rolling_std_14\n",
      "101 rolling_skew_14\n",
      "102 rolling_kurt_14\n",
      "103 rolling_std_21\n",
      "104 rolling_skew_21\n",
      "105 rolling_kurt_21\n",
      "106 day_of_week\n",
      "107 month\n",
      "108 week_of_year\n",
      "109 day_of_year\n",
      "110 quarter\n",
      "111 is_month_start\n",
      "112 is_month_end\n",
      "113 is_quarter_start\n",
      "114 is_quarter_end\n",
      "115 is_year_start\n",
      "116 is_year_end\n",
      "117 season\n",
      "118 close_to_sma_3\n",
      "119 high_to_sma_3\n",
      "120 low_to_sma_3\n",
      "121 open_to_sma_3\n",
      "122 close_to_sma_5\n",
      "123 high_to_sma_5\n",
      "124 low_to_sma_5\n",
      "125 open_to_sma_5\n",
      "126 close_to_sma_7\n",
      "127 high_to_sma_7\n",
      "128 low_to_sma_7\n",
      "129 open_to_sma_7\n",
      "130 close_to_sma_10\n",
      "131 high_to_sma_10\n",
      "132 low_to_sma_10\n",
      "133 open_to_sma_10\n",
      "134 close_to_sma_15\n",
      "135 high_to_sma_15\n",
      "136 low_to_sma_15\n",
      "137 open_to_sma_15\n",
      "138 close_to_sma_20\n",
      "139 high_to_sma_20\n",
      "140 low_to_sma_20\n",
      "141 open_to_sma_20\n",
      "142 close_to_sma_30\n",
      "143 high_to_sma_30\n",
      "144 low_to_sma_30\n",
      "145 open_to_sma_30\n",
      "146 close_to_sma_40\n",
      "147 high_to_sma_40\n",
      "148 low_to_sma_40\n",
      "149 open_to_sma_40\n",
      "150 close_to_sma_50\n",
      "151 high_to_sma_50\n",
      "152 low_to_sma_50\n",
      "153 open_to_sma_50\n",
      "154 close_to_sma_70\n",
      "155 high_to_sma_70\n",
      "156 low_to_sma_70\n",
      "157 open_to_sma_70\n",
      "158 close_to_sma_100\n",
      "159 high_to_sma_100\n",
      "160 low_to_sma_100\n",
      "161 open_to_sma_100\n",
      "162 close_to_sma_150\n",
      "163 high_to_sma_150\n",
      "164 low_to_sma_150\n",
      "165 open_to_sma_150\n",
      "166 close_to_sma_200\n",
      "167 high_to_sma_200\n",
      "168 low_to_sma_200\n",
      "169 open_to_sma_200\n",
      "170 is_split_day\n",
      "171 sma70_above_sma200\n",
      "172 sma70_cross_sma200\n",
      "173 days_since_sma70_cross_200\n",
      "174 sma50_above_sma200\n",
      "175 sma50_cross_sma200\n",
      "176 days_since_sma50_cross_200\n",
      "177 sma20_above_sma50\n",
      "178 sma20_cross_sma50\n",
      "179 days_since_sma20_cross_50\n",
      "180 sma7_above_sma15\n",
      "181 sma7_cross_sma15\n",
      "182 days_since_sma7_cross_15\n",
      "183 sma3_above_sma10\n",
      "184 sma3_cross_sma10\n",
      "185 days_since_sma3_cross_10\n",
      "186 sma3_above_sma7\n",
      "187 sma3_cross_sma7\n",
      "188 days_since_sma3_cross_7\n",
      "189 adx_trend_strength\n",
      "190 macd_state\n",
      "191 macd_cross_signal\n",
      "192 macd_hist_acceleration\n",
      "193 bb_upper_breakout\n",
      "194 bb_lower_breakout\n",
      "195 bb_percent_b\n",
      "196 cbr_rate_change_value\n",
      "197 cbr_rate_change_flag\n",
      "198 momentum_3d\n",
      "199 momentum_sharpe_3d\n",
      "200 momentum_5d\n",
      "201 momentum_sharpe_5d\n",
      "202 momentum_7d\n",
      "203 momentum_sharpe_7d\n",
      "204 momentum_10d\n",
      "205 momentum_sharpe_10d\n",
      "206 momentum_14d\n",
      "207 momentum_sharpe_14d\n",
      "208 momentum_21d\n",
      "209 momentum_sharpe_21d\n",
      "210 momentum_30d\n",
      "211 momentum_sharpe_30d\n",
      "212 momentum_60d\n",
      "213 momentum_sharpe_60d\n",
      "214 momentum_100d\n",
      "215 momentum_sharpe_100d\n",
      "216 momentum_rank_3d\n",
      "217 momentum_rank_5d\n",
      "218 momentum_rank_7d\n",
      "219 momentum_rank_10d\n",
      "220 momentum_rank_14d\n",
      "221 momentum_rank_21d\n",
      "222 momentum_rank_30d\n",
      "223 momentum_rank_60d\n",
      "224 momentum_rank_100d\n"
     ]
    }
   ],
   "source": [
    "print(\"!!! All columns\")\n",
    "\n",
    "for i, name in enumerate(data_with_features.columns):\n",
    "    print(i, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-modeling-TB8BmMSm-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
