{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2e8010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beilakaliev/Library/Caches/pypoetry/virtualenvs/ml-modeling-TB8BmMSm-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pywt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from joblib import Parallel, delayed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11874937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ШАГ 1: ручная карта подтвержденных сплитов ---\n",
    "def load_split_map(filepath: str) -> dict:\n",
    "    \"\"\"Загружает карту сплитов из JSON-файла.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            split_map = json.load(f)\n",
    "        print(f\"Карта сплитов успешно загружена из: {filepath}\")\n",
    "        return split_map\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Предупреждение: Файл с картой сплитов не найден по пути {filepath}. Корректировка не будет произведена.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"ОШИКА: Не удалось прочитать JSON-файл {filepath}. Проверьте его формат.\")\n",
    "        return {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20bc9736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # --- # ToDo!!!! Не реализовано!!! пока что ошибка\n",
    "# def generate_features_grouped(df, column='Close', group_col='Ticker', \n",
    "#                             fft_n=10, wavelet='db4', wavelet_level=3, \n",
    "#                             window=128, step=1):\n",
    "#     all_feats = []\n",
    "\n",
    "#     for ticker, group in df.groupby(group_col):\n",
    "#         series = group[column]\n",
    "#         index = group.index\n",
    "\n",
    "#         feats = []\n",
    "#         for i in range(window, len(series), step):\n",
    "#             # seg = series[i-window:i].values\n",
    "#             # seg = series[i-window:i].to_numpy(dtype=float)\n",
    "#             seg = np.array(series[i-window:i], dtype=np.float64, copy=True)\n",
    "#             ts_idx = index[i]\n",
    "\n",
    "#             feat = {group_col: ticker}\n",
    "\n",
    "#             # Фурье-признаки\n",
    "#             fft_vals = np.fft.fft(seg)\n",
    "#             fft_abs = np.abs(fft_vals[:fft_n])\n",
    "#             for j in range(fft_n):\n",
    "#                 feat[f'fft_{j}'] = fft_abs[j]\n",
    "\n",
    "#             # Вейвлет-признаки\n",
    "#             coeffs = pywt.wavedec(seg, wavelet=wavelet, level=wavelet_level)\n",
    "#             for j, c in enumerate(coeffs):\n",
    "#                 feat[f'wv_L{j}_energy'] = np.sum(np.square(c))\n",
    "#                 feat[f'wv_L{j}_std'] = np.std(c)\n",
    "#                 feat[f'wv_L{j}_mean'] = np.mean(c)\n",
    "\n",
    "#             # STL-декомпозиция\n",
    "#             stl = STL(seg, period=14)\n",
    "#             res = stl.fit()\n",
    "#             feat['stl_trend_last'] = res.trend[-1]\n",
    "#             feat['stl_seasonal_last'] = res.seasonal[-1]\n",
    "#             feat['stl_resid_last'] = res.resid[-1]\n",
    "\n",
    "#             feats.append((ts_idx, feat))\n",
    "\n",
    "#         # Превращаем в DataFrame\n",
    "#         feature_df = pd.DataFrame([f[1] for f in feats], index=[f[0] for f in feats])\n",
    "#         all_feats.append(feature_df)\n",
    "\n",
    "#     # Объединяем все тикеры\n",
    "#     return pd.concat(all_feats)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# ==========================================================\n",
    "# ВСПОМОГАТЕЛЬНАЯ ФУНКЦИЯ: извлечение признаков из сегмента\n",
    "# ==========================================================\n",
    "def extract_transformations_features(seg, fft_n=10, wavelet=\"db4\", wavelet_level=3, stl_period=14):\n",
    "    feat = {}\n",
    "    \n",
    "    seg = np.array(seg, dtype=float).copy()\n",
    "\n",
    "    # --- Фурье ---\n",
    "    fft_vals = np.fft.fft(seg)\n",
    "    fft_abs = np.abs(fft_vals[:fft_n])\n",
    "    for i, val in enumerate(fft_abs):\n",
    "        feat[f'fft_abs_{i}'] = val\n",
    "\n",
    "    # --- Вейвлет ---\n",
    "    coeffs = pywt.wavedec(seg, wavelet=wavelet, level=wavelet_level)\n",
    "    for j, c in enumerate(coeffs):\n",
    "        feat[f'wv_L{j}_energy'] = np.sum(np.square(c))\n",
    "        feat[f'wv_L{j}_std'] = np.std(c)\n",
    "        feat[f'wv_L{j}_mean'] = np.mean(c)\n",
    "\n",
    "    # --- STL ---\n",
    "    try:\n",
    "        stl = STL(seg, period=stl_period, robust=True)\n",
    "        res = stl.fit()\n",
    "        feat['stl_trend_last'] = res.trend[-1]\n",
    "        feat['stl_seasonal_last'] = res.seasonal[-1]\n",
    "        feat['stl_resid_last'] = res.resid[-1]\n",
    "    except Exception:\n",
    "        feat['stl_trend_last'] = np.nan\n",
    "        feat['stl_seasonal_last'] = np.nan\n",
    "        feat['stl_resid_last'] = np.nan\n",
    "\n",
    "    return feat\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# обработка одного тикера\n",
    "# -------------------\n",
    "def _transformations_process_one_group(grp, ticker, column, fft_n, wavelet, wavelet_level, stl_period, window, step):\n",
    "    grp = grp.sort_values(\"Date\").reset_index(drop=True)\n",
    "    feats = pd.DataFrame(index=grp.index)\n",
    "\n",
    "    for i in range(window, len(grp), step):\n",
    "        seg = grp[column].iloc[i-window:i].values\n",
    "        feat = extract_transformations_features(seg, fft_n=fft_n, wavelet=wavelet,\n",
    "                                wavelet_level=wavelet_level, stl_period=stl_period)\n",
    "        for k, v in feat.items():\n",
    "            feats.loc[i, k] = v\n",
    "\n",
    "    feats[\"Ticker\"] = ticker\n",
    "    feats[\"Date\"] = grp[\"Date\"]\n",
    "    print(f\"transformations done -> {ticker}\")\n",
    "    return feats\n",
    "\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# главная функция с параллелью\n",
    "# -------------------\n",
    "def generate_transformations_features_grouped(df, column='Close', group_col='Ticker',\n",
    "                              fft_n=10, wavelet='db4', wavelet_level=3,\n",
    "                              window=128, step=1, stl_period=14, n_jobs=-1):\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(_transformations_process_one_group)(\n",
    "            grp, ticker, column, fft_n, wavelet, wavelet_level, stl_period, window, step\n",
    "        )\n",
    "        for ticker, grp in df.groupby(group_col)\n",
    "    )\n",
    "\n",
    "    feature_df = pd.concat(results, ignore_index=True)\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc1d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# РАСШИРЕННЫЙ ФИЧАИНЖИНИРИНГ (ПОЛНАЯ ИНТЕГРИРОВАННАЯ ВЕРСИЯ)\n",
    "# ==============================================================================\n",
    "\n",
    "def add_features_extended(df: pd.DataFrame, split_map: dict):\n",
    "    \"\"\"\n",
    "    Добавляет в DataFrame расширенный набор технических индикаторов и статистических признаков.\n",
    "    \"\"\"\n",
    "    print(\"Начало расширенного фичаинжиниринга...\")\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values(by=['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "    # --- 1. Признаки тренда (Trend Features) ---\n",
    "    print(\"Расчет индикаторов тренда...\")\n",
    "    sma_periods = [3, 5, 7, 10, 15, 20, 30, 40, 50, 70, 100, 150, 200]\n",
    "    for i in sma_periods:\n",
    "        df[f\"sma_{i}\"] = df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(i).mean())\n",
    "\n",
    "    macd = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.macd(x['Close'], fast=12, slow=26, signal=9))\n",
    "    df = pd.concat([df, macd], axis=1)\n",
    "    adx = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.adx(x['High'], x['Low'], x['Close'], length=14))\n",
    "    df = pd.concat([df, adx], axis=1)\n",
    "\n",
    "    # --- 2. Признаки моментума (Momentum Features) ---\n",
    "    print(\"Расчет индикаторов моментума...\")\n",
    "    rsi_periods = [5, 7, 14, 21, 30, 50]\n",
    "    for i in rsi_periods:\n",
    "        df[f\"rsi_{i}\"] = df.groupby('Ticker')['Close'].transform(lambda x: ta.rsi(x, length=i))\n",
    "\n",
    "    stoch = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.stoch(x['High'], x['Low'], x['Close'], k=14, d=3, smooth_k=3))\n",
    "    df = pd.concat([df, stoch], axis=1)\n",
    "    willr_periods = [5, 7, 14, 21, 30]\n",
    "    for i in willr_periods:\n",
    "        df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
    "\n",
    "    # --- 3. Признаки волатильности (Volatility Features) ---\n",
    "    print(\"Расчет индикаторов волатильности...\")\n",
    "    atr_periods = [5, 7, 14, 21]\n",
    "    for i in atr_periods:\n",
    "        atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n",
    "        df[f\"atr_{i}\"] = atr\n",
    "\n",
    "    bollinger = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.bbands(x['Close'], length=20, std=2))\n",
    "    \n",
    "    df = pd.concat([df, bollinger], axis=1)\n",
    "    df['bb_width_norm'] = (df['BBU_20_2.0_2.0'] - df['BBL_20_2.0_2.0']) / (df['BBM_20_2.0_2.0'] + 1e-9)\n",
    "\n",
    "    # --- 4. Признаки объема (Volume Features) ---\n",
    "    print(\"Расчет индикаторов объема...\")\n",
    "    vol_sma_periods = [5, 7, 14, 20, 30]\n",
    "    for i in vol_sma_periods:\n",
    "        df[f\"vol_sma_{i}\"] = df.groupby('Ticker')['Volume'].transform(lambda x: x.rolling(i).mean())\n",
    "        df[f'relative_volume_{i}'] = df['Volume'] / (df[f\"vol_sma_{i}\"] + 1e-9)\n",
    "\n",
    "    print(\"Расчет и нормализация OBV...\")\n",
    "    obv_series = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.obv(x['Close'], x['Volume']))\n",
    "    df['obv'] = obv_series \n",
    "    for i in vol_sma_periods:\n",
    "        obv_sma_col = f'obv_sma_{i}'\n",
    "        df[obv_sma_col] = df.groupby('Ticker')['obv'].transform(lambda x: x.rolling(i).mean())\n",
    "        df[f'obv_relative_{i}'] = df['obv'] / (df[obv_sma_col] + 1e-9)        \n",
    "        df[f'obv_trend_{i}'] = df['obv'] - df[obv_sma_col]\n",
    "\n",
    "\n",
    "    df['turnover'] = df['Close'] * df['Volume']\n",
    "    for i in vol_sma_periods: # Используем те же периоды для сопоставимости\n",
    "        df[f\"turnover_sma_{i}\"] = df.groupby('Ticker')['turnover'].transform(lambda x: x.rolling(i).mean())\n",
    "        df[f'relative_turnover_{i}'] = df['turnover'] / (df[f\"turnover_sma_{i}\"] + 1e-9)\n",
    "    df.drop(columns=['turnover'], inplace=True)\n",
    "\n",
    "    # --- 5. Признаки свечей и меж-дневной динамики ---\n",
    "    print(\"Расчет признаков свечей и меж-дневной динамики...\")\n",
    "    df['day_range_norm'] = (df['High'] - df['Low']) / (df['Close'] + 1e-9)\n",
    "    df['intraday_move_norm'] = (df['Close'] - df['Open']) / (df['Close'] + 1e-9)\n",
    "    df['upper_wick_norm'] = (df['High'] - df[['Open', 'Close']].max(axis=1)) / (df['Close'] + 1e-9)\n",
    "    df['lower_wick_norm'] = (df[['Open', 'Close']].min(axis=1) - df['Low']) / (df['Close'] + 1e-9)\n",
    "    df['overnight_gap_norm'] = (df['Open'] - df.groupby('Ticker')['Close'].shift(1)) / (df.groupby('Ticker')['Close'].shift(1) + 1e-9)\n",
    "    daily_range = df['High'] - df['Low']\n",
    "    df['range_expansion_ratio'] = daily_range / (df.groupby('Ticker')['High'].shift(1) - df.groupby('Ticker')['Low'].shift(1) + 1e-9)\n",
    "\n",
    "    # --- 6. Признаки взаимодействия Цены и Объема ---\n",
    "    print(\"Расчет признаков взаимодействия Цены и Объема...\")\n",
    "    # Используем relative_volume_20, так как он соответствует периоду Bollinger Bands\n",
    "    df['volume_weighted_move'] = df['intraday_move_norm'] * df['relative_volume_20']\n",
    "    df['daily_return'] = df.groupby('Ticker')['Close'].pct_change()\n",
    "    df['up_day_volume'] = df.apply(lambda row: row['Volume'] if row['daily_return'] > 0 else 0, axis=1)\n",
    "    df['down_day_volume'] = df.apply(lambda row: row['Volume'] if row['daily_return'] <= 0 else 0, axis=1)\n",
    "    \n",
    "    sma_up_vol = df.groupby('Ticker')['up_day_volume'].transform(lambda x: x.rolling(20).mean())\n",
    "    sma_down_vol = df.groupby('Ticker')['down_day_volume'].transform(lambda x: x.rolling(20).mean())\n",
    "    df['up_down_volume_ratio'] = sma_up_vol / (sma_down_vol + 1e-9)\n",
    "    df.drop(columns=['daily_return', 'up_day_volume', 'down_day_volume'], inplace=True)\n",
    "\n",
    "    # --- 7. Статистические признаки ---\n",
    "    print(\"Расчет статистических признаков...\")\n",
    "    df['log_return'] = df.groupby('Ticker')['Close'].transform(lambda x: np.log(x / x.shift(1)))\n",
    "    stat_periods = [7, 14, 21]\n",
    "    for i in stat_periods:\n",
    "        df[f'rolling_std_{i}'] = df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(i).std())\n",
    "        df[f'rolling_skew_{i}'] = df.groupby('Ticker')['log_return'].transform(lambda x: x.rolling(i).skew())\n",
    "        df[f'rolling_kurt_{i}'] = df.groupby('Ticker')['log_return'].transform(lambda x: x.rolling(i).kurt())\n",
    "\n",
    "\n",
    "    # # --- # ToDo!!!! Не реализовано!!! пока что ошибка\n",
    "    # print(\"Расчет Переобразование Фурье, Вейвлет-признаки, STL...\")\n",
    "    # feature_df = generate_features_grouped(df, column='Close', group_col='Ticker', \n",
    "    #                                     fft_n=10, wavelet='db4', wavelet_level=3, \n",
    "    #                                     window=128, step=1)\n",
    "    # df = df.merge(feature_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "    print(\"Расчет Преобразование Фурье, Вейвлет-признаки, STL...\")\n",
    "\n",
    "    feature_df = generate_transformations_features_grouped(\n",
    "        df,\n",
    "        column='Close',\n",
    "        group_col='Ticker',\n",
    "        fft_n=10,\n",
    "        wavelet='db4',\n",
    "        wavelet_level=3,\n",
    "        window=128,\n",
    "        step=5\n",
    "    )\n",
    "    df = df.merge(feature_df, on=['Ticker', 'Date'], how='left')\n",
    "\n",
    "\n",
    "    # --- 8. Календарные признаки ---\n",
    "    print(\"Добавление календарных признаков...\")\n",
    "    df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['week_of_year'] = df['Date'].dt.isocalendar().week.astype(int)\n",
    "    df['day_of_year'] = df['Date'].dt.dayofyear\n",
    "    df['quarter'] = df['Date'].dt.quarter\n",
    "    df['is_month_start'] = df['Date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['Date'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['Date'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['Date'].dt.is_quarter_end.astype(int)\n",
    "    df['is_year_start'] = df['Date'].dt.is_year_start.astype(int)\n",
    "    df['is_year_end'] = df['Date'].dt.is_year_end.astype(int)\n",
    "    def _get_season(month):\n",
    "        if month in [12, 1, 2]: return 0 # Winter\n",
    "        elif month in [3, 4, 5]: return 1 # Spring\n",
    "        elif month in [6, 7, 8]: return 2 # Summer\n",
    "        else: return 3 # Autumn\n",
    "    df['season'] = df['month'].apply(_get_season)\n",
    "    \n",
    "    # --- 9. Событийные признаки и взаимодействия ---\n",
    "    print(\"Расчет признаков взаимодействия и событий...\")\n",
    "    # Ваш полный набор признаков взаимодействия\n",
    "    for i in sma_periods:\n",
    "        sma_col = f'sma_{i}'\n",
    "        df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
    "        df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
    "        df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
    "        df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
    "\n",
    "    # Признак сплита (событийный)\n",
    "    df['is_split_day'] = 0\n",
    "    for ticker, events in split_map.items():\n",
    "        for event in events:\n",
    "            event_date = pd.to_datetime(event['date'])\n",
    "            idx = df[(df['Ticker'] == ticker) & (df['Date'] == event_date)].index\n",
    "            if not idx.empty:\n",
    "                df.loc[idx, 'is_split_day'] = 1\n",
    "    print(f\"Найдено и отмечено {df['is_split_day'].sum()} дней со сплитами.\")\n",
    "\n",
    "\n",
    "    # --- НОВЫЙ РАЗДЕЛ 10: ПРИЗНАКИ ПЕРЕСЕЧЕНИЯ СКОЛЬЗЯЩИХ СРЕДНИХ ---\n",
    "    print(\"Расчет признаков пересечения скользящих средних...\")\n",
    "\n",
    "        \n",
    "    # Определяем пары для анализа (быстрая, медленная)\n",
    "    sma_cross_pairs = [\n",
    "        (70, 200), # Классическое \"Золотое/Мертвое\" пересечение\n",
    "        (50, 200), \n",
    "        (20, 50),  # Среднесрочное пересечение\n",
    "        # Краткосрочные пересечения\n",
    "        (7, 15),\n",
    "        (3, 10),\n",
    "        (3, 7),\n",
    "    ]\n",
    "\n",
    "    for fast_period, slow_period in sma_cross_pairs:\n",
    "        fast_col = f'sma_{fast_period}'\n",
    "        slow_col = f'sma_{slow_period}'\n",
    "        \n",
    "        # Убедимся, что нужные SMA уже посчитаны\n",
    "        if fast_col not in df.columns or slow_col not in df.columns:\n",
    "            raise ValueError(f\"NO sma {fast_col = }. {slow_col = }\")\n",
    "    \n",
    "        # --- Признак 2: Состояние тренда ---\n",
    "        state_col = f'sma{fast_period}_above_sma{slow_period}'\n",
    "        df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
    "        \n",
    "        # --- Признак 1: Сигнал пересечения ---\n",
    "        signal_col = f'sma{fast_period}_cross_sma{slow_period}'\n",
    "        # Сдвигаем состояние на 1 день назад, чтобы сравнить \"сегодня\" и \"вчера\"\n",
    "        prev_state = df.groupby('Ticker')[state_col].shift(1)\n",
    "        # Пересечение - это когда состояние изменилось (0->1 или 1->0)\n",
    "        df[signal_col] = 0\n",
    "        # Бычье пересечение (+1): было 0, стало 1\n",
    "        df.loc[(df[state_col] == 1) & (prev_state == 0), signal_col] = 1\n",
    "        # Медвежье пересечение (-1): было 1, стало 0\n",
    "        df.loc[(df[state_col] == 0) & (prev_state == 1), signal_col] = -1\n",
    "\n",
    "        # --- Признак 3: Дни с момента пересечения ---\n",
    "        days_since_col = f'days_since_sma{fast_period}_cross_{slow_period}'\n",
    "        # Находим, где были пересечения (не равно 0)\n",
    "        cross_events = df[signal_col].ne(0)\n",
    "        # Создаем группы, которые начинаются с каждого пересечения\n",
    "        cross_groups = cross_events.cumsum()\n",
    "        # Считаем дни внутри каждой группы\n",
    "        df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
    "\n",
    "\n",
    "    # --- 11. Продвинутые сигналы технического анализа ---\n",
    "    print(\"Расчет продвинутых сигналов теханализа...\")\n",
    "\n",
    "    # 1. Сигналы ADX (Average Directional Index)\n",
    "    # Что это: ADX показывает СИЛУ тренда (не направление). Пересечение линий +DI и -DI показывает НАПРАВЛЕНИЕ.    \n",
    "    adx_col = 'ADX_14'\n",
    "    dmp_col = 'DMP_14' # +DI\n",
    "    dmn_col = 'DMN_14' # -DI\n",
    "    \n",
    "    # Убедимся, что колонки существуют\n",
    "    if not (adx_col in df.columns and dmp_col in df.columns and dmn_col in df.columns):\n",
    "        raise ValueError(f\"ERROR no {adx_col = }. {dmp_col = }, { dmn_col = }\")\n",
    "        # Признак \"Сила направленного движения\": ADX, умноженный на знак тренда.\n",
    "        # Знак тренда = +1, если +DI выше -DI (бычий), и -1, если наоборот.\n",
    "    trend_direction = (df[dmp_col] > df[dmn_col]).astype(int) * 2 - 1 # Преобразует True/False в +1/-1\n",
    "    df['adx_trend_strength'] = df[adx_col] * trend_direction\n",
    "\n",
    "    # 2. Сигналы MACD (Moving Average Convergence Divergence)\n",
    "    # Что это: Пересечение линии MACD с ее сигнальной линией - классический сигнал.\n",
    "    # Гистограмма (разница между линиями) показывает силу моментума.\n",
    "    macd_line_col = 'MACD_12_26_9'\n",
    "    signal_line_col = 'MACDs_12_26_9'\n",
    "    hist_col = 'MACDh_12_26_9'\n",
    "\n",
    "    if not (macd_line_col in df.columns and signal_line_col in df.columns):\n",
    "        raise ValueError(f\"NO {macd_line_col = }. {signal_line_col = }\")\n",
    "\n",
    "    # Состояние MACD: +1 если MACD выше сигнальной линии (бычье), -1 если ниже (медвежье)\n",
    "    df['macd_state'] = (df[macd_line_col] > df[signal_line_col]).astype(int) * 2 - 1\n",
    "    \n",
    "    # Сигнал пересечения MACD (+1 = бычье, -1 = медвежье)\n",
    "    prev_macd_state = df.groupby('Ticker')['macd_state'].shift(1)\n",
    "    df['macd_cross_signal'] = 0\n",
    "    df.loc[(df['macd_state'] == 1) & (prev_macd_state == -1), 'macd_cross_signal'] = 1\n",
    "    df.loc[(df['macd_state'] == -1) & (prev_macd_state == 1), 'macd_cross_signal'] = -1\n",
    "    \n",
    "    # Признак \"Ускорение моментума\": растет ли гистограмма?\n",
    "    df['macd_hist_acceleration'] = (df[hist_col] > df.groupby('Ticker')[hist_col].shift(1)).astype(int)\n",
    "\n",
    "\n",
    "    # 3. Сигналы по Полосам Боллинджера (Bollinger Bands)\n",
    "    # Что это: Касание или пробой границ канала - сильный сигнал.\n",
    "    upper_bb_col = 'BBU_20_2.0_2.0'\n",
    "    lower_bb_col = 'BBL_20_2.0_2.0'\n",
    "    \n",
    "    if not (upper_bb_col in df.columns and lower_bb_col in df.columns):\n",
    "        raise ValueError(f\"NO { upper_bb_col =}. {lower_bb_col = }\")\n",
    "    # Признак \"Пробой верхней границы\"\n",
    "    df['bb_upper_breakout'] = (df['Close'] > df[upper_bb_col]).astype(int)\n",
    "    # Признак \"Пробой нижней границы\"\n",
    "    df['bb_lower_breakout'] = (df['Close'] < df[lower_bb_col]).astype(int)\n",
    "    # Положение цены внутри канала (от 0 до 1)\n",
    "    # 0 = на нижней границе, 1 = на верхней границе, >1 = пробой вверх, <0 = пробой вниз\n",
    "    df['bb_percent_b'] = (df['Close'] - df[lower_bb_col]) / (df[upper_bb_col] - df[lower_bb_col] + 1e-9)\n",
    "\n",
    "\n",
    "    print(\"Генерация признаков на основе ставки ЦБ...\")    \n",
    "    if 'cbr_rate' in df.columns:\n",
    "        # 1. Величина изменения ставки (рассчитывается внутри каждой группы тикеров)\n",
    "        # .transform() применяет операцию к группе и возвращает результат того же размера,\n",
    "        # что и исходный DataFrame, избегая смешивания данных.\n",
    "        rate_change = df.groupby('Ticker')['cbr_rate'].transform(lambda x: x.replace(-1, np.nan).diff())        \n",
    "        df['cbr_rate_change_value'] = rate_change.fillna(0)\n",
    "\n",
    "        # 2. Факт изменения ставки (1 - было изменение, 0 - не было)\n",
    "        df['cbr_rate_change_flag'] = (df['cbr_rate_change_value'] != 0).astype(int)\n",
    "        \n",
    "    else:\n",
    "        print(\"Предупреждение: Колонка 'cbr_rate' не найдена. Признаки на ее основе не будут созданы.\")\n",
    "        raise Exception\n",
    "\n",
    "\n",
    "    # ---  Признаки Моментума и Относительной Силы ---\n",
    "    print(\"Расчет признаков моментума и относительной силы...\")\n",
    "\n",
    "    # Периоды для анализа\n",
    "    momentum_periods = [3, 5, 7, 10, 14, 21, 30, 60, 100]\n",
    "\n",
    "    for n in momentum_periods:\n",
    "        # Рассчитывается внутри каждого тикера\n",
    "        df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
    "            lambda x: x.pct_change(periods=n)\n",
    "        )        \n",
    "        # --- Моментум, скорректированный на риск (Sharpe Ratio тренда) ---\n",
    "        # log_return уже должен быть рассчитан в секции статистических признаков\n",
    "        if not ('log_return' in df.columns):\n",
    "            raise ValueError(\"No log_return\")\n",
    "        returns_grouped = df.groupby('Ticker')['log_return']\n",
    "        mean_returns = returns_grouped.transform(lambda x: x.rolling(n).mean())\n",
    "        std_returns = returns_grouped.transform(lambda x: x.rolling(n).std())\n",
    "        df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
    "\n",
    "    # ---: Кросс-секционный моментум (Ранжирование) ---\n",
    "    # Этот расчет должен идти после цикла, так как он работает со всеми тикерами одновременно\n",
    "    # для каждой конкретной даты.\n",
    "    print(\"Расчет кросс-секционного ранжирования по моментуму...\")\n",
    "    for n in momentum_periods:\n",
    "        # groupby('Date') - ключевой шаг. Ранжируем акции ВНУТРИ каждого дня.\n",
    "        # rank(pct=True) - преобразует ранг в процентиль (от 0.0 до 1.0), \n",
    "        # что является лучшей практикой для ML моделей.\n",
    "        df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- 10. Финальная очистка от NaN ---\n",
    "    print(\"Очистка данных от NaN...\")\n",
    "    # Находим самый длинный период из всех использованных\n",
    "    longest_period = max(sma_periods)\n",
    "    print(f\"Удаление первых {longest_period} строк для каждого тикера для прогрева индикаторов...\")\n",
    "    # Отбрасываем N первых строк для КАЖДОГО тикера\n",
    "    df = df.groupby('Ticker', group_keys=False).apply(lambda x: x.iloc[longest_period:])\n",
    "    # Дополнительно убираем строки, если где-то остались NaN (например, из-за .shift() в новых признаках)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    print(\"Расширенный фичаинжиниринг завершен.\")\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b48a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка сырых данных из: ../data/moex_normalized_data.csv\n",
      "Данные успешно загружены.\n",
      "Карта сплитов успешно загружена из: config/splits.json\n",
      "Начало расширенного фичаинжиниринга...\n",
      "Расчет индикаторов тренда...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:19: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  macd = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.macd(x['Close'], fast=12, slow=26, signal=9))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:21: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  adx = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.adx(x['High'], x['Low'], x['Close'], length=14))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет индикаторов моментума...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stoch = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.stoch(x['High'], x['Low'], x['Close'], k=14, d=3, smooth_k=3))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:34: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:34: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:34: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:34: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:34: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет индикаторов волатильности...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:43: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bollinger = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.bbands(x['Close'], length=20, std=2))\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:56: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  obv_series = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.obv(x['Close'], x['Volume']))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет индикаторов объема...\n",
      "Расчет и нормализация OBV...\n",
      "Расчет признаков свечей и меж-дневной динамики...\n",
      "Расчет признаков взаимодействия Цены и Объема...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:85: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df['daily_return'] = df.groupby('Ticker')['Close'].pct_change()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет статистических признаков...\n",
      "Расчет Преобразование Фурье, Вейвлет-признаки, STL...\n",
      "transformations done -> ASTR\n",
      "transformations done -> ENPG\n",
      "transformations done -> CBOM\n",
      "transformations done -> AKRN\n",
      "transformations done -> BSPB\n",
      "transformations done -> BANE\n",
      "transformations done -> CHMF\n",
      "transformations done -> BANEP\n",
      "transformations done -> AFKS\n",
      "transformations done -> HEAD\n",
      "transformations done -> ALRS\n",
      "transformations done -> AFLT\n",
      "transformations done -> LEAS\n",
      "transformations done -> FLOT\n",
      "transformations done -> FEES\n",
      "transformations done -> GMKN\n",
      "transformations done -> GAZP\n",
      "transformations done -> IRKT\n",
      "transformations done -> KMAZ\n",
      "transformations done -> HYDR\n",
      "transformations done -> GCHE\n",
      "transformations done -> IRAO\n",
      "transformations done -> LKOH\n",
      "transformations done -> MDMG\n",
      "transformations done -> LSRG\n",
      "transformations done -> MAGN\n",
      "transformations done -> MOEX\n",
      "transformations done -> MSTT\n",
      "transformations done -> MSNG\n",
      "transformations done -> MTLR\n",
      "transformations done -> MGNT\n",
      "transformations done -> MSRS\n",
      "transformations done -> MTSS\n",
      "transformations done -> MTLRP\n",
      "transformations done -> MVID\n",
      "transformations done -> NKNC\n",
      "transformations done -> OZON\n",
      "transformations done -> POSI\n",
      "transformations done -> NLMK\n",
      "transformations done -> RENI\n",
      "transformations done -> NMTP\n",
      "transformations done -> OGKB\n",
      "transformations done -> NVTK\n",
      "transformations done -> PLZL\n",
      "transformations done -> PHOR\n",
      "transformations done -> QIWI\n",
      "transformations done -> PIKK\n",
      "transformations done -> RASP\n",
      "transformations done -> RNFT\n",
      "transformations done -> SGZH\n",
      "transformations done -> ROSN\n",
      "transformations done -> RUAL\n",
      "transformations done -> RTKM\n",
      "transformations done -> SFIN\n",
      "transformations done -> RTKMP\n",
      "transformations done -> SAGO\n",
      "transformations done -> T\n",
      "transformations done -> SELG\n",
      "transformations done -> SBERP\n",
      "transformations done -> SVCB\n",
      "transformations done -> SBER\n",
      "transformations done -> SMLT\n",
      "transformations done -> UGLD\n",
      "transformations done -> SIBN\n",
      "transformations done -> SNGS\n",
      "transformations done -> UWGN\n",
      "transformations done -> SNGSP\n",
      "transformations done -> X5\n",
      "transformations done -> UPRO\n",
      "transformations done -> VKCO\n",
      "transformations done -> YDEX\n",
      "transformations done -> SVAV\n",
      "transformations done -> TRMK\n",
      "transformations done -> TATN\n",
      "transformations done -> TRNFP\n",
      "transformations done -> TATNP\n",
      "transformations done -> VSMO\n",
      "transformations done -> VTBR\n",
      "Добавление календарных признаков...\n",
      "Расчет признаков взаимодействия и событий...\n",
      "Найдено и отмечено 4 дней со сплитами.\n",
      "Расчет признаков пересечения скользящих средних...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['day_of_week'] = df['Date'].dt.dayofweek\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['month'] = df['Date'].dt.month\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['week_of_year'] = df['Date'].dt.isocalendar().week.astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['day_of_year'] = df['Date'].dt.dayofyear\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:132: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['quarter'] = df['Date'].dt.quarter\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_month_start'] = df['Date'].dt.is_month_start.astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:134: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_month_end'] = df['Date'].dt.is_month_end.astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_quarter_start'] = df['Date'].dt.is_quarter_start.astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_quarter_end'] = df['Date'].dt.is_quarter_end.astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_year_start'] = df['Date'].dt.is_year_start.astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_year_end'] = df['Date'].dt.is_year_end.astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['season'] = df['month'].apply(_get_season)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:157: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_split_day'] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:230: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['adx_trend_strength'] = df[adx_col] * trend_direction\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:243: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['macd_state'] = (df[macd_line_col] > df[signal_line_col]).astype(int) * 2 - 1\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:247: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['macd_cross_signal'] = 0\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:252: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['macd_hist_acceleration'] = (df[hist_col] > df.groupby('Ticker')[hist_col].shift(1)).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:263: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['bb_upper_breakout'] = (df['Close'] > df[upper_bb_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['bb_lower_breakout'] = (df['Close'] < df[lower_bb_col]).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:268: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['bb_percent_b'] = (df['Close'] - df[lower_bb_col]) / (df[upper_bb_col] - df[lower_bb_col] + 1e-9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет продвинутых сигналов теханализа...\n",
      "Генерация признаков на основе ставки ЦБ...\n",
      "Расчет признаков моментума и относительной силы...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:277: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['cbr_rate_change_value'] = rate_change.fillna(0)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:280: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['cbr_rate_change_flag'] = (df['cbr_rate_change_value'] != 0).astype(int)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:305: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:305: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:305: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:305: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:305: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:305: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:305: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:305: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:305: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:315: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:315: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет кросс-секционного ранжирования по моментуму...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:315: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:315: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:315: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:315: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:315: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:315: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:315: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Очистка данных от NaN...\n",
      "Удаление первых 200 строк для каждого тикера для прогрева индикаторов...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/9k36l4vn35v4qnj9z669pmtc0000gn/T/ipykernel_88204/1517693826.py:326: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Ticker', group_keys=False).apply(lambda x: x.iloc[longest_period:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расширенный фичаинжиниринг завершен.\n",
      "\n",
      "--- Фильтрация признаков по списку исключений ---\n",
      "Загружен список из 5 признаков для исключения.\n",
      "Будет исключено 5 признаков: ['is_year_start', 'is_year_end', 'is_split_day', 'obv', 'cbr_rate']\n",
      "\n",
      "--- DataFrame с признаками ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34775 entries, 0 to 34774\n",
      "Columns: 245 entries, Date to momentum_rank_100d\n",
      "dtypes: datetime64[ns](1), float64(209), int32(4), int64(30), object(1)\n",
      "memory usage: 64.5+ MB\n",
      "None\n",
      "\n",
      "Сохранение данных с признаками в файл: ../data/moex_with_features.csv\n",
      "Скрипт фичаинжиниринга выполнен успешно!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_folder = \"../data/\"\n",
    "normalized_data_filename = 'moex_normalized_data.csv'\n",
    "features_data_filename = 'moex_with_features.csv'\n",
    "\n",
    "config_folder = \"config/\"\n",
    "split_map_filename = os.path.join(config_folder, \"splits.json\")\n",
    "\n",
    "exclude_list_filename = os.path.join(config_folder, \"feature_exclude_list.json\") # Путь к \"черному списку\"\n",
    "\n",
    "# --- ШАГ 1: Загрузка сырых данных ---\n",
    "print(f\"Загрузка сырых данных из: {data_folder + normalized_data_filename}\")\n",
    "try:\n",
    "    raw_data = pd.read_csv(data_folder + normalized_data_filename)\n",
    "    print(\"Данные успешно загружены.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ОШИБКА: Файл не найден. Убедитесь, что скрипт сохранения сырых данных был запущен.\")\n",
    "    exit()\n",
    "\n",
    "split_map = load_split_map(split_map_filename)\n",
    "\n",
    "# --- ШАГ 2: Добавление признаков ---\n",
    "# tqdm.pandas(desc=\"Расчет индикаторов\")\n",
    "# data_with_features = add_features(raw_data)\n",
    "data_with_features = add_features_extended(raw_data, split_map)\n",
    "\n",
    "\n",
    "\n",
    "# --- ЭТАП 2: ФИЛЬТРАЦИЯ ПО \"ЧЕРНОМУ СПИСКУ\" ---\n",
    "print(\"\\n--- Фильтрация признаков по списку исключений ---\")\n",
    "\n",
    "features_to_exclude = []\n",
    "try:\n",
    "    with open(exclude_list_filename, 'r', encoding='utf-8') as f:\n",
    "        features_to_exclude = json.load(f)\n",
    "    print(f\"Загружен список из {len(features_to_exclude)} признаков для исключения.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ИНФО: Файл исключений '{exclude_list_filename}' не найден. Все признаки будут сохранены.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"ПРЕДУПРЕЖДЕНИЕ: Не удалось прочитать JSON из '{exclude_list_filename}'. Все признаки будут сохранены.\")\n",
    "\n",
    "if features_to_exclude:\n",
    "    # Находим, какие из признаков в списке реально есть в DataFrame\n",
    "    cols_to_drop = [col for col in features_to_exclude if col in data_with_features.columns]\n",
    "    \n",
    "    if cols_to_drop:\n",
    "        print(f\"Будет исключено {len(cols_to_drop)} признаков: {cols_to_drop}\")\n",
    "        final_df = data_with_features.drop(columns=cols_to_drop)\n",
    "    else:\n",
    "        print(\"Ни один из признаков в списке исключений не найден в DataFrame.\")\n",
    "        final_df = data_with_features\n",
    "else:\n",
    "    final_df = data_with_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- DataFrame с признаками ---\")\n",
    "print(final_df.info())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- ШАГ 3: Сохранение результата ---\n",
    "print(f\"\\nСохранение данных с признаками в файл: {data_folder + features_data_filename}\")\n",
    "final_df.to_csv(data_folder + features_data_filename, index=False)\n",
    "\n",
    "print(\"Скрипт фичаинжиниринга выполнен успешно!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff6020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4efd238e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>cbr_rate</th>\n",
       "      <th>sma_3</th>\n",
       "      <th>sma_5</th>\n",
       "      <th>...</th>\n",
       "      <th>momentum_sharpe_100d</th>\n",
       "      <th>momentum_rank_3d</th>\n",
       "      <th>momentum_rank_5d</th>\n",
       "      <th>momentum_rank_7d</th>\n",
       "      <th>momentum_rank_10d</th>\n",
       "      <th>momentum_rank_14d</th>\n",
       "      <th>momentum_rank_21d</th>\n",
       "      <th>momentum_rank_30d</th>\n",
       "      <th>momentum_rank_60d</th>\n",
       "      <th>momentum_rank_100d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>17.90</td>\n",
       "      <td>18.02</td>\n",
       "      <td>17.52</td>\n",
       "      <td>17.75</td>\n",
       "      <td>12653600.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.683333</td>\n",
       "      <td>17.478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017035</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.381818</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.490909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-04-08</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>17.20</td>\n",
       "      <td>17.25</td>\n",
       "      <td>16.52</td>\n",
       "      <td>16.61</td>\n",
       "      <td>16864900.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.180000</td>\n",
       "      <td>17.408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026075</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.618182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-04-15</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>16.40</td>\n",
       "      <td>17.15</td>\n",
       "      <td>16.37</td>\n",
       "      <td>17.12</td>\n",
       "      <td>18057600.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.723333</td>\n",
       "      <td>16.638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029701</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04-22</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>16.43</td>\n",
       "      <td>16.48</td>\n",
       "      <td>16.13</td>\n",
       "      <td>16.25</td>\n",
       "      <td>10607800.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.373333</td>\n",
       "      <td>16.704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>0.618182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-29</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>16.24</td>\n",
       "      <td>16.24</td>\n",
       "      <td>15.85</td>\n",
       "      <td>16.00</td>\n",
       "      <td>8177400.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.056667</td>\n",
       "      <td>16.026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028941</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.327273</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.763636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34770</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4380.00</td>\n",
       "      <td>4420.00</td>\n",
       "      <td>4359.00</td>\n",
       "      <td>4410.00</td>\n",
       "      <td>486636.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4373.500000</td>\n",
       "      <td>4383.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027679</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.294872</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.717949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34771</th>\n",
       "      <td>2025-08-22</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4299.50</td>\n",
       "      <td>4359.00</td>\n",
       "      <td>4265.00</td>\n",
       "      <td>4342.00</td>\n",
       "      <td>469457.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4337.500000</td>\n",
       "      <td>4391.400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001679</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.320513</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.641026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34772</th>\n",
       "      <td>2025-08-29</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4311.00</td>\n",
       "      <td>4333.00</td>\n",
       "      <td>4275.50</td>\n",
       "      <td>4282.50</td>\n",
       "      <td>339398.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4311.500000</td>\n",
       "      <td>4331.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043506</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.602564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34773</th>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4303.00</td>\n",
       "      <td>4349.50</td>\n",
       "      <td>4301.50</td>\n",
       "      <td>4341.00</td>\n",
       "      <td>311666.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4310.166667</td>\n",
       "      <td>4300.200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020832</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.628205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34774</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4312.00</td>\n",
       "      <td>4312.00</td>\n",
       "      <td>4171.50</td>\n",
       "      <td>4200.00</td>\n",
       "      <td>673681.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4273.500000</td>\n",
       "      <td>4320.100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022856</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.628205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34775 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date Ticker     Open     High      Low    Close      Volume  \\\n",
       "0     2015-04-01   AFKS    17.90    18.02    17.52    17.75  12653600.0   \n",
       "1     2015-04-08   AFKS    17.20    17.25    16.52    16.61  16864900.0   \n",
       "2     2015-04-15   AFKS    16.40    17.15    16.37    17.12  18057600.0   \n",
       "3     2015-04-22   AFKS    16.43    16.48    16.13    16.25  10607800.0   \n",
       "4     2015-04-29   AFKS    16.24    16.24    15.85    16.00   8177400.0   \n",
       "...          ...    ...      ...      ...      ...      ...         ...   \n",
       "34770 2025-08-15   YDEX  4380.00  4420.00  4359.00  4410.00    486636.0   \n",
       "34771 2025-08-22   YDEX  4299.50  4359.00  4265.00  4342.00    469457.0   \n",
       "34772 2025-08-29   YDEX  4311.00  4333.00  4275.50  4282.50    339398.0   \n",
       "34773 2025-09-05   YDEX  4303.00  4349.50  4301.50  4341.00    311666.0   \n",
       "34774 2025-09-12   YDEX  4312.00  4312.00  4171.50  4200.00    673681.0   \n",
       "\n",
       "       cbr_rate        sma_3     sma_5  ...  momentum_sharpe_100d  \\\n",
       "0          14.0    17.683333    17.478  ...              0.017035   \n",
       "1          14.0    17.180000    17.408  ...              0.026075   \n",
       "2          14.0    16.723333    16.638  ...              0.029701   \n",
       "3          14.0    16.373333    16.704  ...              0.022446   \n",
       "4          14.0    16.056667    16.026  ...              0.028941   \n",
       "...         ...          ...       ...  ...                   ...   \n",
       "34770      18.0  4373.500000  4383.900  ...             -0.027679   \n",
       "34771      18.0  4337.500000  4391.400  ...             -0.001679   \n",
       "34772      18.0  4311.500000  4331.000  ...              0.043506   \n",
       "34773      18.0  4310.166667  4300.200  ...              0.020832   \n",
       "34774      18.0  4273.500000  4320.100  ...             -0.022856   \n",
       "\n",
       "       momentum_rank_3d  momentum_rank_5d  momentum_rank_7d  \\\n",
       "0              0.418182          0.381818          0.872727   \n",
       "1              0.035714          0.035714          0.107143   \n",
       "2              0.625000          0.767857          0.428571   \n",
       "3              0.053571          0.125000          0.267857   \n",
       "4              0.464286          0.392857          0.196429   \n",
       "...                 ...               ...               ...   \n",
       "34770          0.564103          0.230769          0.384615   \n",
       "34771          0.512821          0.666667          0.756410   \n",
       "34772          0.179487          0.397436          0.512821   \n",
       "34773          0.564103          0.846154          0.769231   \n",
       "34774          0.410256          0.346154          0.397436   \n",
       "\n",
       "       momentum_rank_10d  momentum_rank_14d  momentum_rank_21d  \\\n",
       "0               0.872727           0.981818           0.654545   \n",
       "1               0.072727           0.400000           0.727273   \n",
       "2               0.285714           0.290909           0.581818   \n",
       "3               0.446429           0.160714           0.272727   \n",
       "4               0.125000           0.392857           0.142857   \n",
       "...                  ...                ...                ...   \n",
       "34770           0.435897           0.294872           0.179487   \n",
       "34771           0.371795           0.538462           0.320513   \n",
       "34772           0.615385           0.423077           0.410256   \n",
       "34773           0.679487           0.512821           0.448718   \n",
       "34774           0.641026           0.410256           0.756410   \n",
       "\n",
       "       momentum_rank_30d  momentum_rank_60d  momentum_rank_100d  \n",
       "0               0.109091           0.909091            0.490909  \n",
       "1               0.454545           0.781818            0.618182  \n",
       "2               0.600000           0.763636            0.800000  \n",
       "3               0.763636           0.690909            0.618182  \n",
       "4               0.327273           0.636364            0.763636  \n",
       "...                  ...                ...                 ...  \n",
       "34770           0.576923           0.589744            0.717949  \n",
       "34771           0.461538           0.576923            0.641026  \n",
       "34772           0.179487           0.576923            0.602564  \n",
       "34773           0.487179           0.717949            0.628205  \n",
       "34774           0.538462           0.653846            0.628205  \n",
       "\n",
       "[34775 rows x 250 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36991423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! All columns\n",
      "0 Date\n",
      "1 Ticker\n",
      "2 Open\n",
      "3 High\n",
      "4 Low\n",
      "5 Close\n",
      "6 Volume\n",
      "7 cbr_rate\n",
      "8 sma_3\n",
      "9 sma_5\n",
      "10 sma_7\n",
      "11 sma_10\n",
      "12 sma_15\n",
      "13 sma_20\n",
      "14 sma_30\n",
      "15 sma_40\n",
      "16 sma_50\n",
      "17 sma_70\n",
      "18 sma_100\n",
      "19 sma_150\n",
      "20 sma_200\n",
      "21 MACD_12_26_9\n",
      "22 MACDh_12_26_9\n",
      "23 MACDs_12_26_9\n",
      "24 ADX_14\n",
      "25 ADXR_14_2\n",
      "26 DMP_14\n",
      "27 DMN_14\n",
      "28 rsi_5\n",
      "29 rsi_7\n",
      "30 rsi_14\n",
      "31 rsi_21\n",
      "32 rsi_30\n",
      "33 rsi_50\n",
      "34 STOCHk_14_3_3\n",
      "35 STOCHd_14_3_3\n",
      "36 STOCHh_14_3_3\n",
      "37 willr_5\n",
      "38 willr_7\n",
      "39 willr_14\n",
      "40 willr_21\n",
      "41 willr_30\n",
      "42 atr_5\n",
      "43 atr_7\n",
      "44 atr_14\n",
      "45 atr_21\n",
      "46 BBL_20_2.0_2.0\n",
      "47 BBM_20_2.0_2.0\n",
      "48 BBU_20_2.0_2.0\n",
      "49 BBB_20_2.0_2.0\n",
      "50 BBP_20_2.0_2.0\n",
      "51 bb_width_norm\n",
      "52 vol_sma_5\n",
      "53 relative_volume_5\n",
      "54 vol_sma_7\n",
      "55 relative_volume_7\n",
      "56 vol_sma_14\n",
      "57 relative_volume_14\n",
      "58 vol_sma_20\n",
      "59 relative_volume_20\n",
      "60 vol_sma_30\n",
      "61 relative_volume_30\n",
      "62 obv\n",
      "63 obv_sma_5\n",
      "64 obv_relative_5\n",
      "65 obv_trend_5\n",
      "66 obv_sma_7\n",
      "67 obv_relative_7\n",
      "68 obv_trend_7\n",
      "69 obv_sma_14\n",
      "70 obv_relative_14\n",
      "71 obv_trend_14\n",
      "72 obv_sma_20\n",
      "73 obv_relative_20\n",
      "74 obv_trend_20\n",
      "75 obv_sma_30\n",
      "76 obv_relative_30\n",
      "77 obv_trend_30\n",
      "78 turnover_sma_5\n",
      "79 relative_turnover_5\n",
      "80 turnover_sma_7\n",
      "81 relative_turnover_7\n",
      "82 turnover_sma_14\n",
      "83 relative_turnover_14\n",
      "84 turnover_sma_20\n",
      "85 relative_turnover_20\n",
      "86 turnover_sma_30\n",
      "87 relative_turnover_30\n",
      "88 day_range_norm\n",
      "89 intraday_move_norm\n",
      "90 upper_wick_norm\n",
      "91 lower_wick_norm\n",
      "92 overnight_gap_norm\n",
      "93 range_expansion_ratio\n",
      "94 volume_weighted_move\n",
      "95 up_down_volume_ratio\n",
      "96 log_return\n",
      "97 rolling_std_7\n",
      "98 rolling_skew_7\n",
      "99 rolling_kurt_7\n",
      "100 rolling_std_14\n",
      "101 rolling_skew_14\n",
      "102 rolling_kurt_14\n",
      "103 rolling_std_21\n",
      "104 rolling_skew_21\n",
      "105 rolling_kurt_21\n",
      "106 fft_abs_0\n",
      "107 fft_abs_1\n",
      "108 fft_abs_2\n",
      "109 fft_abs_3\n",
      "110 fft_abs_4\n",
      "111 fft_abs_5\n",
      "112 fft_abs_6\n",
      "113 fft_abs_7\n",
      "114 fft_abs_8\n",
      "115 fft_abs_9\n",
      "116 wv_L0_energy\n",
      "117 wv_L0_std\n",
      "118 wv_L0_mean\n",
      "119 wv_L1_energy\n",
      "120 wv_L1_std\n",
      "121 wv_L1_mean\n",
      "122 wv_L2_energy\n",
      "123 wv_L2_std\n",
      "124 wv_L2_mean\n",
      "125 wv_L3_energy\n",
      "126 wv_L3_std\n",
      "127 wv_L3_mean\n",
      "128 stl_trend_last\n",
      "129 stl_seasonal_last\n",
      "130 stl_resid_last\n",
      "131 day_of_week\n",
      "132 month\n",
      "133 week_of_year\n",
      "134 day_of_year\n",
      "135 quarter\n",
      "136 is_month_start\n",
      "137 is_month_end\n",
      "138 is_quarter_start\n",
      "139 is_quarter_end\n",
      "140 is_year_start\n",
      "141 is_year_end\n",
      "142 season\n",
      "143 close_to_sma_3\n",
      "144 high_to_sma_3\n",
      "145 low_to_sma_3\n",
      "146 open_to_sma_3\n",
      "147 close_to_sma_5\n",
      "148 high_to_sma_5\n",
      "149 low_to_sma_5\n",
      "150 open_to_sma_5\n",
      "151 close_to_sma_7\n",
      "152 high_to_sma_7\n",
      "153 low_to_sma_7\n",
      "154 open_to_sma_7\n",
      "155 close_to_sma_10\n",
      "156 high_to_sma_10\n",
      "157 low_to_sma_10\n",
      "158 open_to_sma_10\n",
      "159 close_to_sma_15\n",
      "160 high_to_sma_15\n",
      "161 low_to_sma_15\n",
      "162 open_to_sma_15\n",
      "163 close_to_sma_20\n",
      "164 high_to_sma_20\n",
      "165 low_to_sma_20\n",
      "166 open_to_sma_20\n",
      "167 close_to_sma_30\n",
      "168 high_to_sma_30\n",
      "169 low_to_sma_30\n",
      "170 open_to_sma_30\n",
      "171 close_to_sma_40\n",
      "172 high_to_sma_40\n",
      "173 low_to_sma_40\n",
      "174 open_to_sma_40\n",
      "175 close_to_sma_50\n",
      "176 high_to_sma_50\n",
      "177 low_to_sma_50\n",
      "178 open_to_sma_50\n",
      "179 close_to_sma_70\n",
      "180 high_to_sma_70\n",
      "181 low_to_sma_70\n",
      "182 open_to_sma_70\n",
      "183 close_to_sma_100\n",
      "184 high_to_sma_100\n",
      "185 low_to_sma_100\n",
      "186 open_to_sma_100\n",
      "187 close_to_sma_150\n",
      "188 high_to_sma_150\n",
      "189 low_to_sma_150\n",
      "190 open_to_sma_150\n",
      "191 close_to_sma_200\n",
      "192 high_to_sma_200\n",
      "193 low_to_sma_200\n",
      "194 open_to_sma_200\n",
      "195 is_split_day\n",
      "196 sma70_above_sma200\n",
      "197 sma70_cross_sma200\n",
      "198 days_since_sma70_cross_200\n",
      "199 sma50_above_sma200\n",
      "200 sma50_cross_sma200\n",
      "201 days_since_sma50_cross_200\n",
      "202 sma20_above_sma50\n",
      "203 sma20_cross_sma50\n",
      "204 days_since_sma20_cross_50\n",
      "205 sma7_above_sma15\n",
      "206 sma7_cross_sma15\n",
      "207 days_since_sma7_cross_15\n",
      "208 sma3_above_sma10\n",
      "209 sma3_cross_sma10\n",
      "210 days_since_sma3_cross_10\n",
      "211 sma3_above_sma7\n",
      "212 sma3_cross_sma7\n",
      "213 days_since_sma3_cross_7\n",
      "214 adx_trend_strength\n",
      "215 macd_state\n",
      "216 macd_cross_signal\n",
      "217 macd_hist_acceleration\n",
      "218 bb_upper_breakout\n",
      "219 bb_lower_breakout\n",
      "220 bb_percent_b\n",
      "221 cbr_rate_change_value\n",
      "222 cbr_rate_change_flag\n",
      "223 momentum_3d\n",
      "224 momentum_sharpe_3d\n",
      "225 momentum_5d\n",
      "226 momentum_sharpe_5d\n",
      "227 momentum_7d\n",
      "228 momentum_sharpe_7d\n",
      "229 momentum_10d\n",
      "230 momentum_sharpe_10d\n",
      "231 momentum_14d\n",
      "232 momentum_sharpe_14d\n",
      "233 momentum_21d\n",
      "234 momentum_sharpe_21d\n",
      "235 momentum_30d\n",
      "236 momentum_sharpe_30d\n",
      "237 momentum_60d\n",
      "238 momentum_sharpe_60d\n",
      "239 momentum_100d\n",
      "240 momentum_sharpe_100d\n",
      "241 momentum_rank_3d\n",
      "242 momentum_rank_5d\n",
      "243 momentum_rank_7d\n",
      "244 momentum_rank_10d\n",
      "245 momentum_rank_14d\n",
      "246 momentum_rank_21d\n",
      "247 momentum_rank_30d\n",
      "248 momentum_rank_60d\n",
      "249 momentum_rank_100d\n"
     ]
    }
   ],
   "source": [
    "print(\"!!! All columns\")\n",
    "\n",
    "for i, name in enumerate(data_with_features.columns):\n",
    "    print(i, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28d75384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>fft_abs_0</th>\n",
       "      <th>fft_abs_1</th>\n",
       "      <th>fft_abs_2</th>\n",
       "      <th>fft_abs_3</th>\n",
       "      <th>fft_abs_4</th>\n",
       "      <th>fft_abs_5</th>\n",
       "      <th>fft_abs_6</th>\n",
       "      <th>fft_abs_7</th>\n",
       "      <th>...</th>\n",
       "      <th>wv_L1_mean</th>\n",
       "      <th>wv_L2_energy</th>\n",
       "      <th>wv_L2_std</th>\n",
       "      <th>wv_L2_mean</th>\n",
       "      <th>wv_L3_energy</th>\n",
       "      <th>wv_L3_std</th>\n",
       "      <th>wv_L3_mean</th>\n",
       "      <th>stl_trend_last</th>\n",
       "      <th>stl_seasonal_last</th>\n",
       "      <th>stl_resid_last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>1818.09</td>\n",
       "      <td>214.215083</td>\n",
       "      <td>77.297263</td>\n",
       "      <td>23.127340</td>\n",
       "      <td>83.391347</td>\n",
       "      <td>51.513621</td>\n",
       "      <td>32.990779</td>\n",
       "      <td>25.137241</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.501008</td>\n",
       "      <td>81.079838</td>\n",
       "      <td>1.476834</td>\n",
       "      <td>-0.101535</td>\n",
       "      <td>25.395406</td>\n",
       "      <td>0.611788</td>\n",
       "      <td>0.068934</td>\n",
       "      <td>16.235754</td>\n",
       "      <td>1.724820</td>\n",
       "      <td>-0.060574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-04-08</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>1818.86</td>\n",
       "      <td>213.997326</td>\n",
       "      <td>76.861187</td>\n",
       "      <td>19.686267</td>\n",
       "      <td>85.600761</td>\n",
       "      <td>44.935683</td>\n",
       "      <td>31.929401</td>\n",
       "      <td>29.043642</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018658</td>\n",
       "      <td>47.385463</td>\n",
       "      <td>1.131610</td>\n",
       "      <td>0.012142</td>\n",
       "      <td>9.011981</td>\n",
       "      <td>0.361887</td>\n",
       "      <td>-0.059541</td>\n",
       "      <td>17.954312</td>\n",
       "      <td>-0.660893</td>\n",
       "      <td>-0.063419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-04-15</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>1826.43</td>\n",
       "      <td>219.018549</td>\n",
       "      <td>70.543785</td>\n",
       "      <td>19.822885</td>\n",
       "      <td>92.139759</td>\n",
       "      <td>38.076893</td>\n",
       "      <td>34.343059</td>\n",
       "      <td>33.154988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121006</td>\n",
       "      <td>26.897930</td>\n",
       "      <td>0.839051</td>\n",
       "      <td>0.151538</td>\n",
       "      <td>21.979837</td>\n",
       "      <td>0.569838</td>\n",
       "      <td>0.057805</td>\n",
       "      <td>17.108023</td>\n",
       "      <td>-0.318865</td>\n",
       "      <td>-0.419158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04-22</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>1838.97</td>\n",
       "      <td>224.764130</td>\n",
       "      <td>66.784106</td>\n",
       "      <td>15.709267</td>\n",
       "      <td>92.359575</td>\n",
       "      <td>33.919610</td>\n",
       "      <td>23.279818</td>\n",
       "      <td>39.135887</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073173</td>\n",
       "      <td>25.821427</td>\n",
       "      <td>0.834452</td>\n",
       "      <td>-0.039572</td>\n",
       "      <td>8.502822</td>\n",
       "      <td>0.352141</td>\n",
       "      <td>-0.053895</td>\n",
       "      <td>16.746477</td>\n",
       "      <td>-0.088079</td>\n",
       "      <td>-0.228398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-29</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>1849.68</td>\n",
       "      <td>227.731570</td>\n",
       "      <td>70.328340</td>\n",
       "      <td>11.883908</td>\n",
       "      <td>84.641506</td>\n",
       "      <td>42.799255</td>\n",
       "      <td>20.112039</td>\n",
       "      <td>31.513323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039115</td>\n",
       "      <td>20.378272</td>\n",
       "      <td>0.741239</td>\n",
       "      <td>-0.036450</td>\n",
       "      <td>20.186095</td>\n",
       "      <td>0.544783</td>\n",
       "      <td>0.067053</td>\n",
       "      <td>15.990645</td>\n",
       "      <td>-0.209899</td>\n",
       "      <td>0.339254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34770</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>545288.00</td>\n",
       "      <td>15707.154061</td>\n",
       "      <td>4490.252555</td>\n",
       "      <td>1389.315861</td>\n",
       "      <td>1042.944498</td>\n",
       "      <td>5419.614530</td>\n",
       "      <td>4773.977933</td>\n",
       "      <td>829.003404</td>\n",
       "      <td>...</td>\n",
       "      <td>7.900908</td>\n",
       "      <td>178261.952582</td>\n",
       "      <td>68.286567</td>\n",
       "      <td>-12.443286</td>\n",
       "      <td>143522.511486</td>\n",
       "      <td>46.078885</td>\n",
       "      <td>4.343206</td>\n",
       "      <td>4254.880905</td>\n",
       "      <td>59.302702</td>\n",
       "      <td>60.316393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34771</th>\n",
       "      <td>2025-08-22</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>544367.00</td>\n",
       "      <td>14890.619605</td>\n",
       "      <td>4378.329498</td>\n",
       "      <td>2024.143333</td>\n",
       "      <td>1841.825534</td>\n",
       "      <td>4685.593208</td>\n",
       "      <td>4100.076604</td>\n",
       "      <td>178.419034</td>\n",
       "      <td>...</td>\n",
       "      <td>-20.274581</td>\n",
       "      <td>220955.081811</td>\n",
       "      <td>76.912228</td>\n",
       "      <td>7.501208</td>\n",
       "      <td>149656.537681</td>\n",
       "      <td>47.237317</td>\n",
       "      <td>-1.521692</td>\n",
       "      <td>4472.858529</td>\n",
       "      <td>-192.042338</td>\n",
       "      <td>9.183809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34772</th>\n",
       "      <td>2025-08-29</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>543031.00</td>\n",
       "      <td>13620.962555</td>\n",
       "      <td>4038.258086</td>\n",
       "      <td>2780.664203</td>\n",
       "      <td>2674.692029</td>\n",
       "      <td>4890.121104</td>\n",
       "      <td>3016.940382</td>\n",
       "      <td>1385.979938</td>\n",
       "      <td>...</td>\n",
       "      <td>21.584460</td>\n",
       "      <td>189889.401990</td>\n",
       "      <td>71.110102</td>\n",
       "      <td>8.689037</td>\n",
       "      <td>134442.122837</td>\n",
       "      <td>44.794885</td>\n",
       "      <td>-0.131000</td>\n",
       "      <td>4544.376020</td>\n",
       "      <td>198.681105</td>\n",
       "      <td>-434.057126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34773</th>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>542243.00</td>\n",
       "      <td>12835.442121</td>\n",
       "      <td>3606.438264</td>\n",
       "      <td>2868.057086</td>\n",
       "      <td>2609.614640</td>\n",
       "      <td>5629.445217</td>\n",
       "      <td>3545.839067</td>\n",
       "      <td>1279.510319</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.489439</td>\n",
       "      <td>167757.125451</td>\n",
       "      <td>66.383562</td>\n",
       "      <td>-11.278255</td>\n",
       "      <td>120215.608322</td>\n",
       "      <td>42.315551</td>\n",
       "      <td>-1.912318</td>\n",
       "      <td>4273.644509</td>\n",
       "      <td>20.741347</td>\n",
       "      <td>5.114144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34774</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>541262.00</td>\n",
       "      <td>11875.571851</td>\n",
       "      <td>2861.788951</td>\n",
       "      <td>2596.976105</td>\n",
       "      <td>1980.674439</td>\n",
       "      <td>5955.359439</td>\n",
       "      <td>4264.745928</td>\n",
       "      <td>415.140465</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.248794</td>\n",
       "      <td>187084.153647</td>\n",
       "      <td>70.338698</td>\n",
       "      <td>-10.430536</td>\n",
       "      <td>118935.904272</td>\n",
       "      <td>42.082964</td>\n",
       "      <td>2.046195</td>\n",
       "      <td>4312.386372</td>\n",
       "      <td>-45.252938</td>\n",
       "      <td>29.866566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34775 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date Ticker  fft_abs_0     fft_abs_1    fft_abs_2    fft_abs_3  \\\n",
       "0     2015-04-01   AFKS    1818.09    214.215083    77.297263    23.127340   \n",
       "1     2015-04-08   AFKS    1818.86    213.997326    76.861187    19.686267   \n",
       "2     2015-04-15   AFKS    1826.43    219.018549    70.543785    19.822885   \n",
       "3     2015-04-22   AFKS    1838.97    224.764130    66.784106    15.709267   \n",
       "4     2015-04-29   AFKS    1849.68    227.731570    70.328340    11.883908   \n",
       "...          ...    ...        ...           ...          ...          ...   \n",
       "34770 2025-08-15   YDEX  545288.00  15707.154061  4490.252555  1389.315861   \n",
       "34771 2025-08-22   YDEX  544367.00  14890.619605  4378.329498  2024.143333   \n",
       "34772 2025-08-29   YDEX  543031.00  13620.962555  4038.258086  2780.664203   \n",
       "34773 2025-09-05   YDEX  542243.00  12835.442121  3606.438264  2868.057086   \n",
       "34774 2025-09-12   YDEX  541262.00  11875.571851  2861.788951  2596.976105   \n",
       "\n",
       "         fft_abs_4    fft_abs_5    fft_abs_6    fft_abs_7  ...  wv_L1_mean  \\\n",
       "0        83.391347    51.513621    32.990779    25.137241  ...   -0.501008   \n",
       "1        85.600761    44.935683    31.929401    29.043642  ...   -0.018658   \n",
       "2        92.139759    38.076893    34.343059    33.154988  ...    0.121006   \n",
       "3        92.359575    33.919610    23.279818    39.135887  ...   -0.073173   \n",
       "4        84.641506    42.799255    20.112039    31.513323  ...    0.039115   \n",
       "...            ...          ...          ...          ...  ...         ...   \n",
       "34770  1042.944498  5419.614530  4773.977933   829.003404  ...    7.900908   \n",
       "34771  1841.825534  4685.593208  4100.076604   178.419034  ...  -20.274581   \n",
       "34772  2674.692029  4890.121104  3016.940382  1385.979938  ...   21.584460   \n",
       "34773  2609.614640  5629.445217  3545.839067  1279.510319  ...   -5.489439   \n",
       "34774  1980.674439  5955.359439  4264.745928   415.140465  ...   -1.248794   \n",
       "\n",
       "        wv_L2_energy  wv_L2_std  wv_L2_mean   wv_L3_energy  wv_L3_std  \\\n",
       "0          81.079838   1.476834   -0.101535      25.395406   0.611788   \n",
       "1          47.385463   1.131610    0.012142       9.011981   0.361887   \n",
       "2          26.897930   0.839051    0.151538      21.979837   0.569838   \n",
       "3          25.821427   0.834452   -0.039572       8.502822   0.352141   \n",
       "4          20.378272   0.741239   -0.036450      20.186095   0.544783   \n",
       "...              ...        ...         ...            ...        ...   \n",
       "34770  178261.952582  68.286567  -12.443286  143522.511486  46.078885   \n",
       "34771  220955.081811  76.912228    7.501208  149656.537681  47.237317   \n",
       "34772  189889.401990  71.110102    8.689037  134442.122837  44.794885   \n",
       "34773  167757.125451  66.383562  -11.278255  120215.608322  42.315551   \n",
       "34774  187084.153647  70.338698  -10.430536  118935.904272  42.082964   \n",
       "\n",
       "       wv_L3_mean  stl_trend_last  stl_seasonal_last  stl_resid_last  \n",
       "0        0.068934       16.235754           1.724820       -0.060574  \n",
       "1       -0.059541       17.954312          -0.660893       -0.063419  \n",
       "2        0.057805       17.108023          -0.318865       -0.419158  \n",
       "3       -0.053895       16.746477          -0.088079       -0.228398  \n",
       "4        0.067053       15.990645          -0.209899        0.339254  \n",
       "...           ...             ...                ...             ...  \n",
       "34770    4.343206     4254.880905          59.302702       60.316393  \n",
       "34771   -1.521692     4472.858529        -192.042338        9.183809  \n",
       "34772   -0.131000     4544.376020         198.681105     -434.057126  \n",
       "34773   -1.912318     4273.644509          20.741347        5.114144  \n",
       "34774    2.046195     4312.386372         -45.252938       29.866566  \n",
       "\n",
       "[34775 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_features[[\n",
    "    \"Date\",\n",
    "    \"Ticker\",\n",
    "\"fft_abs_0\",\n",
    "\"fft_abs_1\",\n",
    "\"fft_abs_2\",\n",
    "\"fft_abs_3\",\n",
    "\"fft_abs_4\",\n",
    "\"fft_abs_5\",\n",
    "\"fft_abs_6\",\n",
    "\"fft_abs_7\",\n",
    "\"fft_abs_8\",\n",
    "\"fft_abs_9\",\n",
    "\"wv_L0_energy\",\n",
    "\"wv_L0_std\",\n",
    "\"wv_L0_mean\",\n",
    "\"wv_L1_energy\",\n",
    "\"wv_L1_std\",\n",
    "\"wv_L1_mean\",\n",
    "\"wv_L2_energy\",\n",
    "\"wv_L2_std\",\n",
    "\"wv_L2_mean\",\n",
    "\"wv_L3_energy\",\n",
    "\"wv_L3_std\",\n",
    "\"wv_L3_mean\",\n",
    "\"stl_trend_last\",\n",
    "\"stl_seasonal_last\",\n",
    "\"stl_resid_last\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd21010",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-modeling-TB8BmMSm-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
