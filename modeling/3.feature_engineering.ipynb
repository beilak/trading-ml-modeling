{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2e8010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rualvb1/self/trading-ml-modeling/.venv/lib/python3.12/site-packages/pandas_ta/__init__.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n",
      "/Users/rualvb1/self/trading-ml-modeling/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11874937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ШАГ 1: Ваша ручная карта подтвержденных сплитов ---\n",
    "def load_split_map(filepath: str) -> dict:\n",
    "    \"\"\"Загружает карту сплитов из JSON-файла.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            split_map = json.load(f)\n",
    "        print(f\"Карта сплитов успешно загружена из: {filepath}\")\n",
    "        return split_map\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Предупреждение: Файл с картой сплитов не найден по пути {filepath}. Корректировка не будет произведена.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"ОШИКА: Не удалось прочитать JSON-файл {filepath}. Проверьте его формат.\")\n",
    "        return {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# РАСШИРЕННЫЙ ФИЧАИНЖИНИРИНГ (ПОЛНАЯ ИНТЕГРИРОВАННАЯ ВЕРСИЯ)\n",
    "# ==============================================================================\n",
    "\n",
    "def add_features_extended(df: pd.DataFrame, split_map: dict):\n",
    "    \"\"\"\n",
    "    Добавляет в DataFrame расширенный набор технических индикаторов и статистических признаков.\n",
    "    \"\"\"\n",
    "    print(\"Начало расширенного фичаинжиниринга...\")\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values(by=['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "    # --- 1. Признаки тренда (Trend Features) ---\n",
    "    print(\"Расчет индикаторов тренда...\")\n",
    "    sma_periods = [3, 5, 7, 10, 15, 20, 30, 40, 50, 70, 100, 150, 200]\n",
    "    for i in sma_periods:\n",
    "        df[f\"sma_{i}\"] = df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(i).mean())\n",
    "\n",
    "    macd = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.macd(x['Close'], fast=12, slow=26, signal=9))\n",
    "    df = pd.concat([df, macd], axis=1)\n",
    "    adx = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.adx(x['High'], x['Low'], x['Close'], length=14))\n",
    "    df = pd.concat([df, adx], axis=1)\n",
    "\n",
    "    # --- 2. Признаки моментума (Momentum Features) ---\n",
    "    print(\"Расчет индикаторов моментума...\")\n",
    "    rsi_periods = [5, 7, 14, 21, 30, 50]\n",
    "    for i in rsi_periods:\n",
    "        df[f\"rsi_{i}\"] = df.groupby('Ticker')['Close'].transform(lambda x: ta.rsi(x, length=i))\n",
    "\n",
    "    stoch = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.stoch(x['High'], x['Low'], x['Close'], k=14, d=3, smooth_k=3))\n",
    "    df = pd.concat([df, stoch], axis=1)\n",
    "    willr_periods = [5, 7, 14, 21, 30]\n",
    "    for i in willr_periods:\n",
    "        df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
    "\n",
    "    # --- 3. Признаки волатильности (Volatility Features) ---\n",
    "    print(\"Расчет индикаторов волатильности...\")\n",
    "    atr_periods = [5, 7, 14, 21]\n",
    "    for i in atr_periods:\n",
    "        atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n",
    "        df[f\"atr_{i}\"] = atr\n",
    "\n",
    "    bollinger = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.bbands(x['Close'], length=20, std=2))\n",
    "    df = pd.concat([df, bollinger], axis=1)\n",
    "    df['bb_width_norm'] = (df['BBU_20_2.0'] - df['BBL_20_2.0']) / (df['BBM_20_2.0'] + 1e-9)\n",
    "\n",
    "    # --- 4. Признаки объема (Volume Features) ---\n",
    "    print(\"Расчет индикаторов объема...\")\n",
    "    vol_sma_periods = [5, 7, 14, 20, 30]\n",
    "    for i in vol_sma_periods:\n",
    "        df[f\"vol_sma_{i}\"] = df.groupby('Ticker')['Volume'].transform(lambda x: x.rolling(i).mean())\n",
    "        df[f'relative_volume_{i}'] = df['Volume'] / (df[f\"vol_sma_{i}\"] + 1e-9)\n",
    "\n",
    "    print(\"Расчет и нормализация OBV...\")\n",
    "    obv_series = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.obv(x['Close'], x['Volume']))\n",
    "    df['obv'] = obv_series \n",
    "    for i in vol_sma_periods:\n",
    "        obv_sma_col = f'obv_sma_{i}'\n",
    "        df[obv_sma_col] = df.groupby('Ticker')['obv'].transform(lambda x: x.rolling(i).mean())\n",
    "        df[f'obv_relative_{i}'] = df['obv'] / (df[obv_sma_col] + 1e-9)        \n",
    "        df[f'obv_trend_{i}'] = df['obv'] - df[obv_sma_col]\n",
    "\n",
    "\n",
    "    df['turnover'] = df['Close'] * df['Volume']\n",
    "    for i in vol_sma_periods: # Используем те же периоды для сопоставимости\n",
    "        df[f\"turnover_sma_{i}\"] = df.groupby('Ticker')['turnover'].transform(lambda x: x.rolling(i).mean())\n",
    "        df[f'relative_turnover_{i}'] = df['turnover'] / (df[f\"turnover_sma_{i}\"] + 1e-9)\n",
    "    df.drop(columns=['turnover'], inplace=True)\n",
    "\n",
    "    # --- 5. Признаки свечей и меж-дневной динамики ---\n",
    "    print(\"Расчет признаков свечей и меж-дневной динамики...\")\n",
    "    df['day_range_norm'] = (df['High'] - df['Low']) / (df['Close'] + 1e-9)\n",
    "    df['intraday_move_norm'] = (df['Close'] - df['Open']) / (df['Close'] + 1e-9)\n",
    "    df['upper_wick_norm'] = (df['High'] - df[['Open', 'Close']].max(axis=1)) / (df['Close'] + 1e-9)\n",
    "    df['lower_wick_norm'] = (df[['Open', 'Close']].min(axis=1) - df['Low']) / (df['Close'] + 1e-9)\n",
    "    df['overnight_gap_norm'] = (df['Open'] - df.groupby('Ticker')['Close'].shift(1)) / (df.groupby('Ticker')['Close'].shift(1) + 1e-9)\n",
    "    daily_range = df['High'] - df['Low']\n",
    "    df['range_expansion_ratio'] = daily_range / (df.groupby('Ticker')['High'].shift(1) - df.groupby('Ticker')['Low'].shift(1) + 1e-9)\n",
    "\n",
    "    # --- 6. Признаки взаимодействия Цены и Объема ---\n",
    "    print(\"Расчет признаков взаимодействия Цены и Объема...\")\n",
    "    # Используем relative_volume_20, так как он соответствует периоду Bollinger Bands\n",
    "    df['volume_weighted_move'] = df['intraday_move_norm'] * df['relative_volume_20']\n",
    "    df['daily_return'] = df.groupby('Ticker')['Close'].pct_change()\n",
    "    df['up_day_volume'] = df.apply(lambda row: row['Volume'] if row['daily_return'] > 0 else 0, axis=1)\n",
    "    df['down_day_volume'] = df.apply(lambda row: row['Volume'] if row['daily_return'] <= 0 else 0, axis=1)\n",
    "    \n",
    "    sma_up_vol = df.groupby('Ticker')['up_day_volume'].transform(lambda x: x.rolling(20).mean())\n",
    "    sma_down_vol = df.groupby('Ticker')['down_day_volume'].transform(lambda x: x.rolling(20).mean())\n",
    "    df['up_down_volume_ratio'] = sma_up_vol / (sma_down_vol + 1e-9)\n",
    "    df.drop(columns=['daily_return', 'up_day_volume', 'down_day_volume'], inplace=True)\n",
    "\n",
    "    # --- 7. Статистические признаки ---\n",
    "    print(\"Расчет статистических признаков...\")\n",
    "    df['log_return'] = df.groupby('Ticker')['Close'].transform(lambda x: np.log(x / x.shift(1)))\n",
    "    stat_periods = [7, 14, 21]\n",
    "    for i in stat_periods:\n",
    "        df[f'rolling_std_{i}'] = df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(i).std())\n",
    "        df[f'rolling_skew_{i}'] = df.groupby('Ticker')['log_return'].transform(lambda x: x.rolling(i).skew())\n",
    "        df[f'rolling_kurt_{i}'] = df.groupby('Ticker')['log_return'].transform(lambda x: x.rolling(i).kurt())\n",
    "\n",
    "    # --- 8. Календарные признаки ---\n",
    "    print(\"Добавление календарных признаков...\")\n",
    "    df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['week_of_year'] = df['Date'].dt.isocalendar().week.astype(int)\n",
    "    df['day_of_year'] = df['Date'].dt.dayofyear\n",
    "    df['quarter'] = df['Date'].dt.quarter\n",
    "    df['is_month_start'] = df['Date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['Date'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['Date'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['Date'].dt.is_quarter_end.astype(int)\n",
    "    df['is_year_start'] = df['Date'].dt.is_year_start.astype(int)\n",
    "    df['is_year_end'] = df['Date'].dt.is_year_end.astype(int)\n",
    "    def _get_season(month):\n",
    "        if month in [12, 1, 2]: return 0 # Winter\n",
    "        elif month in [3, 4, 5]: return 1 # Spring\n",
    "        elif month in [6, 7, 8]: return 2 # Summer\n",
    "        else: return 3 # Autumn\n",
    "    df['season'] = df['month'].apply(_get_season)\n",
    "    \n",
    "    # --- 9. Событийные признаки и взаимодействия ---\n",
    "    print(\"Расчет признаков взаимодействия и событий...\")\n",
    "    # Ваш полный набор признаков взаимодействия\n",
    "    for i in sma_periods:\n",
    "        sma_col = f'sma_{i}'\n",
    "        df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
    "        df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
    "        df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
    "        df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
    "\n",
    "    # Признак сплита (событийный)\n",
    "    df['is_split_day'] = 0\n",
    "    for ticker, events in split_map.items():\n",
    "        for event in events:\n",
    "            event_date = pd.to_datetime(event['date'])\n",
    "            idx = df[(df['Ticker'] == ticker) & (df['Date'] == event_date)].index\n",
    "            if not idx.empty:\n",
    "                df.loc[idx, 'is_split_day'] = 1\n",
    "    print(f\"Найдено и отмечено {df['is_split_day'].sum()} дней со сплитами.\")\n",
    "\n",
    "\n",
    "    # --- НОВЫЙ РАЗДЕЛ 10: ПРИЗНАКИ ПЕРЕСЕЧЕНИЯ СКОЛЬЗЯЩИХ СРЕДНИХ ---\n",
    "    print(\"Расчет признаков пересечения скользящих средних...\")\n",
    "\n",
    "        \n",
    "    # Определяем пары для анализа (быстрая, медленная)\n",
    "    sma_cross_pairs = [\n",
    "        (70, 200), # Классическое \"Золотое/Мертвое\" пересечение\n",
    "        (50, 200), \n",
    "        (20, 50),  # Среднесрочное пересечение\n",
    "        # Краткосрочные пересечения\n",
    "        (7, 15),\n",
    "        (3, 10),\n",
    "        (3, 7),\n",
    "    ]\n",
    "\n",
    "    for fast_period, slow_period in sma_cross_pairs:\n",
    "        fast_col = f'sma_{fast_period}'\n",
    "        slow_col = f'sma_{slow_period}'\n",
    "        \n",
    "        # Убедимся, что нужные SMA уже посчитаны\n",
    "        if fast_col not in df.columns or slow_col not in df.columns:\n",
    "            raise ValueError(f\"NO sma {fast_col = }. {slow_col = }\")\n",
    "    \n",
    "        # --- Признак 2: Состояние тренда ---\n",
    "        state_col = f'sma{fast_period}_above_sma{slow_period}'\n",
    "        df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
    "        \n",
    "        # --- Признак 1: Сигнал пересечения ---\n",
    "        signal_col = f'sma{fast_period}_cross_sma{slow_period}'\n",
    "        # Сдвигаем состояние на 1 день назад, чтобы сравнить \"сегодня\" и \"вчера\"\n",
    "        prev_state = df.groupby('Ticker')[state_col].shift(1)\n",
    "        # Пересечение - это когда состояние изменилось (0->1 или 1->0)\n",
    "        df[signal_col] = 0\n",
    "        # Бычье пересечение (+1): было 0, стало 1\n",
    "        df.loc[(df[state_col] == 1) & (prev_state == 0), signal_col] = 1\n",
    "        # Медвежье пересечение (-1): было 1, стало 0\n",
    "        df.loc[(df[state_col] == 0) & (prev_state == 1), signal_col] = -1\n",
    "\n",
    "        # --- Признак 3: Дни с момента пересечения ---\n",
    "        days_since_col = f'days_since_sma{fast_period}_cross_{slow_period}'\n",
    "        # Находим, где были пересечения (не равно 0)\n",
    "        cross_events = df[signal_col].ne(0)\n",
    "        # Создаем группы, которые начинаются с каждого пересечения\n",
    "        cross_groups = cross_events.cumsum()\n",
    "        # Считаем дни внутри каждой группы\n",
    "        df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
    "\n",
    "\n",
    "    # --- 11. Продвинутые сигналы технического анализа ---\n",
    "    print(\"Расчет продвинутых сигналов теханализа...\")\n",
    "\n",
    "    # 1. Сигналы ADX (Average Directional Index)\n",
    "    # Что это: ADX показывает СИЛУ тренда (не направление). Пересечение линий +DI и -DI показывает НАПРАВЛЕНИЕ.    \n",
    "    adx_col = 'ADX_14'\n",
    "    dmp_col = 'DMP_14' # +DI\n",
    "    dmn_col = 'DMN_14' # -DI\n",
    "    \n",
    "    # Убедимся, что колонки существуют\n",
    "    if not (adx_col in df.columns and dmp_col in df.columns and dmn_col in df.columns):\n",
    "        raise ValueError(f\"ERROR no {adx_col = }. {dmp_col = }, { dmn_col = }\")\n",
    "        # Признак \"Сила направленного движения\": ADX, умноженный на знак тренда.\n",
    "        # Знак тренда = +1, если +DI выше -DI (бычий), и -1, если наоборот.\n",
    "    trend_direction = (df[dmp_col] > df[dmn_col]).astype(int) * 2 - 1 # Преобразует True/False в +1/-1\n",
    "    df['adx_trend_strength'] = df[adx_col] * trend_direction\n",
    "\n",
    "    # 2. Сигналы MACD (Moving Average Convergence Divergence)\n",
    "    # Что это: Пересечение линии MACD с ее сигнальной линией - классический сигнал.\n",
    "    # Гистограмма (разница между линиями) показывает силу моментума.\n",
    "    macd_line_col = 'MACD_12_26_9'\n",
    "    signal_line_col = 'MACDs_12_26_9'\n",
    "    hist_col = 'MACDh_12_26_9'\n",
    "\n",
    "    if not (macd_line_col in df.columns and signal_line_col in df.columns):\n",
    "        raise ValueError(f\"NO {macd_line_col = }. {signal_line_col = }\")\n",
    "\n",
    "    # Состояние MACD: +1 если MACD выше сигнальной линии (бычье), -1 если ниже (медвежье)\n",
    "    df['macd_state'] = (df[macd_line_col] > df[signal_line_col]).astype(int) * 2 - 1\n",
    "    \n",
    "    # Сигнал пересечения MACD (+1 = бычье, -1 = медвежье)\n",
    "    prev_macd_state = df.groupby('Ticker')['macd_state'].shift(1)\n",
    "    df['macd_cross_signal'] = 0\n",
    "    df.loc[(df['macd_state'] == 1) & (prev_macd_state == -1), 'macd_cross_signal'] = 1\n",
    "    df.loc[(df['macd_state'] == -1) & (prev_macd_state == 1), 'macd_cross_signal'] = -1\n",
    "    \n",
    "    # Признак \"Ускорение моментума\": растет ли гистограмма?\n",
    "    df['macd_hist_acceleration'] = (df[hist_col] > df.groupby('Ticker')[hist_col].shift(1)).astype(int)\n",
    "\n",
    "\n",
    "    # 3. Сигналы по Полосам Боллинджера (Bollinger Bands)\n",
    "    # Что это: Касание или пробой границ канала - сильный сигнал.\n",
    "    upper_bb_col = 'BBU_20_2.0'\n",
    "    lower_bb_col = 'BBL_20_2.0'\n",
    "    \n",
    "    if not (upper_bb_col in df.columns and lower_bb_col in df.columns):\n",
    "        raise ValueError(f\"NO { upper_bb_col =}. {lower_bb_col = }\")\n",
    "    # Признак \"Пробой верхней границы\"\n",
    "    df['bb_upper_breakout'] = (df['Close'] > df[upper_bb_col]).astype(int)\n",
    "    # Признак \"Пробой нижней границы\"\n",
    "    df['bb_lower_breakout'] = (df['Close'] < df[lower_bb_col]).astype(int)\n",
    "    # Положение цены внутри канала (от 0 до 1)\n",
    "    # 0 = на нижней границе, 1 = на верхней границе, >1 = пробой вверх, <0 = пробой вниз\n",
    "    df['bb_percent_b'] = (df['Close'] - df[lower_bb_col]) / (df[upper_bb_col] - df[lower_bb_col] + 1e-9)\n",
    "\n",
    "\n",
    "    print(\"Генерация признаков на основе ставки ЦБ...\")    \n",
    "    if 'cbr_rate' in df.columns:\n",
    "        # 1. Величина изменения ставки (рассчитывается внутри каждой группы тикеров)\n",
    "        # .transform() применяет операцию к группе и возвращает результат того же размера,\n",
    "        # что и исходный DataFrame, избегая смешивания данных.\n",
    "        rate_change = df.groupby('Ticker')['cbr_rate'].transform(lambda x: x.replace(-1, np.nan).diff())        \n",
    "        df['cbr_rate_change_value'] = rate_change.fillna(0)\n",
    "\n",
    "        # 2. Факт изменения ставки (1 - было изменение, 0 - не было)\n",
    "        df['cbr_rate_change_flag'] = (df['cbr_rate_change_value'] != 0).astype(int)\n",
    "        \n",
    "    else:\n",
    "        print(\"Предупреждение: Колонка 'cbr_rate' не найдена. Признаки на ее основе не будут созданы.\")\n",
    "        raise Exception\n",
    "\n",
    "\n",
    "    # ---  Признаки Моментума и Относительной Силы ---\n",
    "    print(\"Расчет признаков моментума и относительной силы...\")\n",
    "\n",
    "    # Периоды для анализа\n",
    "    momentum_periods = [3, 5, 7, 10, 14, 21, 30, 60, 100]\n",
    "\n",
    "    for n in momentum_periods:\n",
    "        # Рассчитывается внутри каждого тикера\n",
    "        df[f'momentum_{n}d'] = df.groupby('Ticker')['Close'].transform(\n",
    "            lambda x: x.pct_change(periods=n)\n",
    "        )        \n",
    "        # --- Моментум, скорректированный на риск (Sharpe Ratio тренда) ---\n",
    "        # log_return уже должен быть рассчитан в секции статистических признаков\n",
    "        if not ('log_return' in df.columns):\n",
    "            raise ValueError(\"No log_return\")\n",
    "        returns_grouped = df.groupby('Ticker')['log_return']\n",
    "        mean_returns = returns_grouped.transform(lambda x: x.rolling(n).mean())\n",
    "        std_returns = returns_grouped.transform(lambda x: x.rolling(n).std())\n",
    "        df[f'momentum_sharpe_{n}d'] = mean_returns / (std_returns + 1e-9)\n",
    "\n",
    "    # ---: Кросс-секционный моментум (Ранжирование) ---\n",
    "    # Этот расчет должен идти после цикла, так как он работает со всеми тикерами одновременно\n",
    "    # для каждой конкретной даты.\n",
    "    print(\"Расчет кросс-секционного ранжирования по моментуму...\")\n",
    "    for n in momentum_periods:\n",
    "        # groupby('Date') - ключевой шаг. Ранжируем акции ВНУТРИ каждого дня.\n",
    "        # rank(pct=True) - преобразует ранг в процентиль (от 0.0 до 1.0), \n",
    "        # что является лучшей практикой для ML моделей.\n",
    "        df[f'momentum_rank_{n}d'] = df.groupby('Date')[f'momentum_{n}d'].rank(pct=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- 10. Финальная очистка от NaN ---\n",
    "    print(\"Очистка данных от NaN...\")\n",
    "    # Находим самый длинный период из всех использованных\n",
    "    longest_period = max(sma_periods)\n",
    "    print(f\"Удаление первых {longest_period} строк для каждого тикера для прогрева индикаторов...\")\n",
    "    # Отбрасываем N первых строк для КАЖДОГО тикера\n",
    "    df = df.groupby('Ticker', group_keys=False).apply(lambda x: x.iloc[longest_period:])\n",
    "    # Дополнительно убираем строки, если где-то остались NaN (например, из-за .shift() в новых признаках)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    print(\"Расширенный фичаинжиниринг завершен.\")\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b48a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка сырых данных из: ../data/moex_normalized_data.csv\n",
      "Данные успешно загружены.\n",
      "Карта сплитов успешно загружена из: config/splits.json\n",
      "Начало расширенного фичаинжиниринга...\n",
      "Расчет индикаторов тренда...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  macd = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.macd(x['Close'], fast=12, slow=26, signal=9))\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  adx = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.adx(x['High'], x['Low'], x['Close'], length=14))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет индикаторов моментума...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stoch = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.stoch(x['High'], x['Low'], x['Close'], k=14, d=3, smooth_k=3))\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"willr_{i}\"] = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.willr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет индикаторов волатильности...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  atr = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.atr(x['High'], x['Low'], x['Close'], length=i))\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bollinger = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.bbands(x['Close'], length=20, std=2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет индикаторов объема...\n",
      "Расчет и нормализация OBV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  obv_series = df.groupby('Ticker', group_keys=False).apply(lambda x: ta.obv(x['Close'], x['Volume']))\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:84: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df['daily_return'] = df.groupby('Ticker')['Close'].pct_change()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет признаков свечей и меж-дневной динамики...\n",
      "Расчет признаков взаимодействия Цены и Объема...\n",
      "Расчет статистических признаков...\n",
      "Добавление календарных признаков...\n",
      "Расчет признаков взаимодействия и событий...\n",
      "Найдено и отмечено 4 дней со сплитами.\n",
      "Расчет признаков пересечения скользящих средних...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:127: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:127: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:127: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:127: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:127: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'close_to_{sma_col}'] = (df['Close'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'high_to_{sma_col}'] = (df['High'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'low_to_{sma_col}'] = (df['Low'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'open_to_{sma_col}'] = (df['Open'] - df[sma_col]) / (df[sma_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['is_split_day'] = 0\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[state_col] = (df[fast_col] > df[slow_col]).astype(int)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[signal_col] = 0\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[days_since_col] = df.groupby(['Ticker', cross_groups]).cumcount()\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:206: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['adx_trend_strength'] = df[adx_col] * trend_direction\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:219: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['macd_state'] = (df[macd_line_col] > df[signal_line_col]).astype(int) * 2 - 1\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:223: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['macd_cross_signal'] = 0\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:228: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['macd_hist_acceleration'] = (df[hist_col] > df.groupby('Ticker')[hist_col].shift(1)).astype(int)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:239: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['bb_upper_breakout'] = (df['Close'] > df[upper_bb_col]).astype(int)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:241: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['bb_lower_breakout'] = (df['Close'] < df[lower_bb_col]).astype(int)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:244: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['bb_percent_b'] = (df['Close'] - df[lower_bb_col]) / (df[upper_bb_col] - df[lower_bb_col] + 1e-9)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:253: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['cbr_rate_change_value'] = rate_change.fillna(0)\n",
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:256: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['cbr_rate_change_flag'] = (df['cbr_rate_change_value'] != 0).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет продвинутых сигналов теханализа...\n",
      "Генерация признаков на основе ставки ЦБ...\n",
      "Очистка данных от NaN...\n",
      "Удаление первых 200 строк для каждого тикера для прогрева индикаторов...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/st88xs4n5dv45285_22l27v9977913/T/ipykernel_22277/613825911.py:269: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Ticker', group_keys=False).apply(lambda x: x.iloc[longest_period:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расширенный фичаинжиниринг завершен.\n",
      "\n",
      "--- Фильтрация признаков по списку исключений ---\n",
      "Загружен список из 4 признаков для исключения.\n",
      "Будет исключено 4 признаков: ['is_year_start', 'is_year_end', 'is_split_day', 'obv']\n",
      "\n",
      "--- DataFrame с признаками ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 171935 entries, 0 to 171934\n",
      "Columns: 192 entries, Date to cbr_rate_change_flag\n",
      "dtypes: datetime64[ns](1), float64(156), int32(4), int64(30), object(1)\n",
      "memory usage: 249.2+ MB\n",
      "None\n",
      "\n",
      "Сохранение данных с признаками в файл: ../data/moex_with_features.csv\n",
      "Скрипт фичаинжиниринга выполнен успешно!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_folder = \"../data/\"\n",
    "normalized_data_filename = 'moex_normalized_data.csv'\n",
    "features_data_filename = 'moex_with_features.csv'\n",
    "\n",
    "config_folder = \"config/\"\n",
    "split_map_filename = os.path.join(config_folder, \"splits.json\")\n",
    "\n",
    "exclude_list_filename = os.path.join(config_folder, \"feature_exclude_list.json\") # Путь к \"черному списку\"\n",
    "\n",
    "# --- ШАГ 1: Загрузка сырых данных ---\n",
    "print(f\"Загрузка сырых данных из: {data_folder + normalized_data_filename}\")\n",
    "try:\n",
    "    raw_data = pd.read_csv(data_folder + normalized_data_filename)\n",
    "    print(\"Данные успешно загружены.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ОШИБКА: Файл не найден. Убедитесь, что скрипт сохранения сырых данных был запущен.\")\n",
    "    exit()\n",
    "\n",
    "split_map = load_split_map(split_map_filename)\n",
    "\n",
    "# --- ШАГ 2: Добавление признаков ---\n",
    "# tqdm.pandas(desc=\"Расчет индикаторов\")\n",
    "# data_with_features = add_features(raw_data)\n",
    "data_with_features = add_features_extended(raw_data, split_map)\n",
    "\n",
    "\n",
    "\n",
    "# --- ЭТАП 2: ФИЛЬТРАЦИЯ ПО \"ЧЕРНОМУ СПИСКУ\" ---\n",
    "print(\"\\n--- Фильтрация признаков по списку исключений ---\")\n",
    "\n",
    "features_to_exclude = []\n",
    "try:\n",
    "    with open(exclude_list_filename, 'r', encoding='utf-8') as f:\n",
    "        features_to_exclude = json.load(f)\n",
    "    print(f\"Загружен список из {len(features_to_exclude)} признаков для исключения.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ИНФО: Файл исключений '{exclude_list_filename}' не найден. Все признаки будут сохранены.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"ПРЕДУПРЕЖДЕНИЕ: Не удалось прочитать JSON из '{exclude_list_filename}'. Все признаки будут сохранены.\")\n",
    "\n",
    "if features_to_exclude:\n",
    "    # Находим, какие из признаков в списке реально есть в DataFrame\n",
    "    cols_to_drop = [col for col in features_to_exclude if col in data_with_features.columns]\n",
    "    \n",
    "    if cols_to_drop:\n",
    "        print(f\"Будет исключено {len(cols_to_drop)} признаков: {cols_to_drop}\")\n",
    "        final_df = data_with_features.drop(columns=cols_to_drop)\n",
    "    else:\n",
    "        print(\"Ни один из признаков в списке исключений не найден в DataFrame.\")\n",
    "        final_df = data_with_features\n",
    "else:\n",
    "    final_df = data_with_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- DataFrame с признаками ---\")\n",
    "print(final_df.info())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- ШАГ 3: Сохранение результата ---\n",
    "print(f\"\\nСохранение данных с признаками в файл: {data_folder + features_data_filename}\")\n",
    "final_df.to_csv(data_folder + features_data_filename, index=False)\n",
    "\n",
    "print(\"Скрипт фичаинжиниринга выполнен успешно!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efd238e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>cbr_rate</th>\n",
       "      <th>sma_3</th>\n",
       "      <th>sma_5</th>\n",
       "      <th>...</th>\n",
       "      <th>days_since_sma3_cross_7</th>\n",
       "      <th>adx_trend_strength</th>\n",
       "      <th>macd_state</th>\n",
       "      <th>macd_cross_signal</th>\n",
       "      <th>macd_hist_acceleration</th>\n",
       "      <th>bb_upper_breakout</th>\n",
       "      <th>bb_lower_breakout</th>\n",
       "      <th>bb_percent_b</th>\n",
       "      <th>cbr_rate_change_value</th>\n",
       "      <th>cbr_rate_change_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-27</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>17.04</td>\n",
       "      <td>17.30</td>\n",
       "      <td>16.62</td>\n",
       "      <td>17.30</td>\n",
       "      <td>9700300.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.313333</td>\n",
       "      <td>17.084</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>21.343528</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.654506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-03-30</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>17.21</td>\n",
       "      <td>17.47</td>\n",
       "      <td>17.08</td>\n",
       "      <td>17.40</td>\n",
       "      <td>13542700.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.246667</td>\n",
       "      <td>17.246</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>20.924588</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.699851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>17.37</td>\n",
       "      <td>17.93</td>\n",
       "      <td>17.10</td>\n",
       "      <td>17.90</td>\n",
       "      <td>17961200.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.533333</td>\n",
       "      <td>17.448</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>21.053356</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>17.90</td>\n",
       "      <td>18.02</td>\n",
       "      <td>17.52</td>\n",
       "      <td>17.75</td>\n",
       "      <td>12653600.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.683333</td>\n",
       "      <td>17.478</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>21.270898</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.803677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-02</td>\n",
       "      <td>AFKS</td>\n",
       "      <td>17.75</td>\n",
       "      <td>18.00</td>\n",
       "      <td>17.12</td>\n",
       "      <td>17.85</td>\n",
       "      <td>21043800.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.833333</td>\n",
       "      <td>17.640</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>20.778789</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.821523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171930</th>\n",
       "      <td>2025-08-12</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4405.00</td>\n",
       "      <td>4430.00</td>\n",
       "      <td>4350.00</td>\n",
       "      <td>4394.00</td>\n",
       "      <td>467519.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4405.833333</td>\n",
       "      <td>4372.300</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>15.817183</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.769417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171931</th>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4409.00</td>\n",
       "      <td>4438.00</td>\n",
       "      <td>4325.00</td>\n",
       "      <td>4336.00</td>\n",
       "      <td>410565.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4378.333333</td>\n",
       "      <td>4384.700</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>15.515821</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.623362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171932</th>\n",
       "      <td>2025-08-14</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4352.50</td>\n",
       "      <td>4407.00</td>\n",
       "      <td>4316.00</td>\n",
       "      <td>4374.50</td>\n",
       "      <td>669147.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4368.166667</td>\n",
       "      <td>4385.600</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>15.103868</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.701936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171933</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4380.00</td>\n",
       "      <td>4420.00</td>\n",
       "      <td>4359.00</td>\n",
       "      <td>4410.00</td>\n",
       "      <td>486636.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4373.500000</td>\n",
       "      <td>4383.900</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>14.883367</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.786714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171934</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>YDEX</td>\n",
       "      <td>4364.00</td>\n",
       "      <td>4509.50</td>\n",
       "      <td>4277.00</td>\n",
       "      <td>4506.00</td>\n",
       "      <td>1238767.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4430.166667</td>\n",
       "      <td>4404.100</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15.665770</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.970894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171935 rows × 196 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date Ticker     Open     High      Low    Close      Volume  \\\n",
       "0      2015-03-27   AFKS    17.04    17.30    16.62    17.30   9700300.0   \n",
       "1      2015-03-30   AFKS    17.21    17.47    17.08    17.40  13542700.0   \n",
       "2      2015-03-31   AFKS    17.37    17.93    17.10    17.90  17961200.0   \n",
       "3      2015-04-01   AFKS    17.90    18.02    17.52    17.75  12653600.0   \n",
       "4      2015-04-02   AFKS    17.75    18.00    17.12    17.85  21043800.0   \n",
       "...           ...    ...      ...      ...      ...      ...         ...   \n",
       "171930 2025-08-12   YDEX  4405.00  4430.00  4350.00  4394.00    467519.0   \n",
       "171931 2025-08-13   YDEX  4409.00  4438.00  4325.00  4336.00    410565.0   \n",
       "171932 2025-08-14   YDEX  4352.50  4407.00  4316.00  4374.50    669147.0   \n",
       "171933 2025-08-15   YDEX  4380.00  4420.00  4359.00  4410.00    486636.0   \n",
       "171934 2025-08-18   YDEX  4364.00  4509.50  4277.00  4506.00   1238767.0   \n",
       "\n",
       "        cbr_rate        sma_3     sma_5  ...  days_since_sma3_cross_7  \\\n",
       "0           14.0    17.313333    17.084  ...                        7   \n",
       "1           14.0    17.246667    17.246  ...                        8   \n",
       "2           14.0    17.533333    17.448  ...                        9   \n",
       "3           14.0    17.683333    17.478  ...                       10   \n",
       "4           14.0    17.833333    17.640  ...                       11   \n",
       "...          ...          ...       ...  ...                      ...   \n",
       "171930      18.0  4405.833333  4372.300  ...                        4   \n",
       "171931      18.0  4378.333333  4384.700  ...                        5   \n",
       "171932      18.0  4368.166667  4385.600  ...                        6   \n",
       "171933      18.0  4373.500000  4383.900  ...                        0   \n",
       "171934      18.0  4430.166667  4404.100  ...                        0   \n",
       "\n",
       "        adx_trend_strength  macd_state  macd_cross_signal  \\\n",
       "0                21.343528           1                  0   \n",
       "1                20.924588           1                  0   \n",
       "2                21.053356           1                  0   \n",
       "3                21.270898           1                  0   \n",
       "4                20.778789           1                  0   \n",
       "...                    ...         ...                ...   \n",
       "171930           15.817183           1                  0   \n",
       "171931           15.515821           1                  0   \n",
       "171932           15.103868           1                  0   \n",
       "171933           14.883367           1                  0   \n",
       "171934           15.665770           1                  0   \n",
       "\n",
       "        macd_hist_acceleration  bb_upper_breakout  bb_lower_breakout  \\\n",
       "0                            1                  0                  0   \n",
       "1                            1                  0                  0   \n",
       "2                            1                  0                  0   \n",
       "3                            1                  0                  0   \n",
       "4                            1                  0                  0   \n",
       "...                        ...                ...                ...   \n",
       "171930                       1                  0                  0   \n",
       "171931                       0                  0                  0   \n",
       "171932                       0                  0                  0   \n",
       "171933                       1                  0                  0   \n",
       "171934                       1                  0                  0   \n",
       "\n",
       "        bb_percent_b  cbr_rate_change_value  cbr_rate_change_flag  \n",
       "0           0.654506                    0.0                     0  \n",
       "1           0.699851                    0.0                     0  \n",
       "2           0.833671                    0.0                     0  \n",
       "3           0.803677                    0.0                     0  \n",
       "4           0.821523                    0.0                     0  \n",
       "...              ...                    ...                   ...  \n",
       "171930      0.769417                    0.0                     0  \n",
       "171931      0.623362                    0.0                     0  \n",
       "171932      0.701936                    0.0                     0  \n",
       "171933      0.786714                    0.0                     0  \n",
       "171934      0.970894                    0.0                     0  \n",
       "\n",
       "[171935 rows x 196 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-modeling-py3.12 (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
